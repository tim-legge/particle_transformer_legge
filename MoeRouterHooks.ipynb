{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "829bc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moe ParT modified to store router output, accessed via MoeRouterHook\n",
    "\n",
    "from typing import List, Optional\n",
    "import timeit\n",
    "import awkward as ak\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter \n",
    "from torch.nn.init import xavier_uniform_, xavier_normal_, constant_\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Optional\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "_is_fastpath_enabled: bool = True\n",
    "from torch.overrides import (\n",
    "    handle_torch_function,\n",
    "    has_torch_function,\n",
    "    has_torch_function_unary,\n",
    "    has_torch_function_variadic,\n",
    ")\n",
    "linear = torch._C._nn.linear\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import copy\n",
    "from torch._C import _add_docstr, _infer_size\n",
    "\n",
    "from functools import partial\n",
    "from weaver.utils.logger import _logger\n",
    "import os\n",
    "import uproot\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch._torch_docs import reproducibility_notes, sparse_support_notes, tf32_notes\n",
    "import mplhep as hep\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "38bd540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to facilitate modded Multihead\n",
    "\n",
    "def _canonical_mask(\n",
    "    mask: Optional[Tensor],\n",
    "    mask_name: str,\n",
    "    other_type: Optional[torch.dtype],\n",
    "    other_name: str,\n",
    "    target_type: torch.dtype,\n",
    "    check_other: bool = True,\n",
    ") -> Optional[Tensor]:\n",
    "    if mask is not None:\n",
    "        _mask_dtype = mask.dtype\n",
    "        _mask_is_float = torch.is_floating_point(mask)\n",
    "        if _mask_dtype != torch.bool and not _mask_is_float:\n",
    "            raise AssertionError(\n",
    "                f\"only bool and floating types of {mask_name} are supported\"\n",
    "            )\n",
    "        if check_other and other_type is not None:\n",
    "            if _mask_dtype != other_type:\n",
    "                warnings.warn(\n",
    "                    f\"Support for mismatched {mask_name} and {other_name} \"\n",
    "                    \"is deprecated. Use same type for both instead.\"\n",
    "                )\n",
    "        if not _mask_is_float:\n",
    "            mask = torch.zeros_like(mask, dtype=target_type).masked_fill_(\n",
    "                mask, float(\"-inf\")\n",
    "            )\n",
    "    return mask\n",
    "def _none_or_dtype(input: Optional[Tensor]) -> Optional[torch.dtype]:\n",
    "    if input is None:\n",
    "        return None\n",
    "    elif isinstance(input, torch.Tensor):\n",
    "        return input.dtype\n",
    "    raise RuntimeError(\"input to _none_or_dtype() must be None or torch.Tensor\")\n",
    "def get_fastpath_enabled() -> bool:\n",
    "    \"\"\"Returns whether fast path for TransformerEncoder and MultiHeadAttention\n",
    "    is enabled, or ``True`` if jit is scripting.\n",
    "\n",
    "    ..note:\n",
    "        The fastpath might not be run even if ``get_fastpath_enabled`` returns\n",
    "        ``True`` unless all conditions on inputs are met.\n",
    "    \"\"\"\n",
    "    if not torch.jit.is_scripting():\n",
    "        return _is_fastpath_enabled\n",
    "    return True\n",
    "\n",
    "def _in_projection_packed(\n",
    "    q: Tensor,\n",
    "    k: Tensor,\n",
    "    v: Tensor,\n",
    "    w: Tensor,\n",
    "    b: Optional[Tensor] = None,\n",
    ") -> List[Tensor]:\n",
    "    r\"\"\"Perform the in-projection step of the attention operation, using packed weights.\n",
    "\n",
    "    Output is a triple containing projection tensors for query, key and value.\n",
    "\n",
    "    Args:\n",
    "        q, k, v: query, key and value tensors to be projected. For self-attention,\n",
    "            these are typically the same tensor; for encoder-decoder attention,\n",
    "            k and v are typically the same tensor. (We take advantage of these\n",
    "            identities for performance if they are present.) Regardless, q, k and v\n",
    "            must share a common embedding dimension; otherwise their shapes may vary.\n",
    "        w: projection weights for q, k and v, packed into a single tensor. Weights\n",
    "            are packed along dimension 0, in q, k, v order.\n",
    "        b: optional projection biases for q, k and v, packed into a single tensor\n",
    "            in q, k, v order.\n",
    "\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - q: :math:`(..., E)` where E is the embedding dimension\n",
    "        - k: :math:`(..., E)` where E is the embedding dimension\n",
    "        - v: :math:`(..., E)` where E is the embedding dimension\n",
    "        - w: :math:`(E * 3, E)` where E is the embedding dimension\n",
    "        - b: :math:`E * 3` where E is the embedding dimension\n",
    "\n",
    "        Output:\n",
    "        - in output list :math:`[q', k', v']`, each output tensor will have the\n",
    "            same shape as the corresponding input tensor.\n",
    "    \"\"\"\n",
    "    E = q.size(-1)\n",
    "    if k is v:\n",
    "        if q is k:\n",
    "            # self-attention\n",
    "            proj = linear(q, w, b)\n",
    "            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            proj = (\n",
    "                proj.unflatten(-1, (3, E))\n",
    "                .unsqueeze(0)\n",
    "                .transpose(0, -2)\n",
    "                .squeeze(-2)\n",
    "                .contiguous()\n",
    "            )\n",
    "            return proj[0], proj[1], proj[2]\n",
    "        else:\n",
    "            # encoder-decoder attention\n",
    "            w_q, w_kv = w.split([E, E * 2])\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "            q_proj = linear(q, w_q, b_q)\n",
    "            kv_proj = linear(k, w_kv, b_kv)\n",
    "            # reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            kv_proj = (\n",
    "                kv_proj.unflatten(-1, (2, E))\n",
    "                .unsqueeze(0)\n",
    "                .transpose(0, -2)\n",
    "                .squeeze(-2)\n",
    "                .contiguous()\n",
    "            )\n",
    "            return (q_proj, kv_proj[0], kv_proj[1])\n",
    "    else:\n",
    "        w_q, w_k, w_v = w.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n",
    "\n",
    "def _mha_shape_check(\n",
    "    query: Tensor,\n",
    "    key: Tensor,\n",
    "    value: Tensor,\n",
    "    key_padding_mask: Optional[Tensor],\n",
    "    attn_mask: Optional[Tensor],\n",
    "    num_heads: int,\n",
    "):\n",
    "    # Verifies the expected shape for `query, `key`, `value`, `key_padding_mask` and `attn_mask`\n",
    "    # and returns if the input is batched or not.\n",
    "    # Raises an error if `query` is not 2-D (unbatched) or 3-D (batched) tensor.\n",
    "\n",
    "    # Shape check.\n",
    "    if query.dim() == 3:\n",
    "        # Batched Inputs\n",
    "        is_batched = True\n",
    "        assert key.dim() == 3 and value.dim() == 3, (\n",
    "            \"For batched (3-D) `query`, expected `key` and `value` to be 3-D\"\n",
    "            f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\"\n",
    "        )\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.dim() == 2, (\n",
    "                \"For batched (3-D) `query`, expected `key_padding_mask` to be `None` or 2-D\"\n",
    "                f\" but found {key_padding_mask.dim()}-D tensor instead\"\n",
    "            )\n",
    "        if attn_mask is not None:\n",
    "            assert attn_mask.dim() in (2, 3), (\n",
    "                \"For batched (3-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n",
    "                f\" but found {attn_mask.dim()}-D tensor instead\"\n",
    "            )\n",
    "    elif query.dim() == 2:\n",
    "        # Unbatched Inputs\n",
    "        is_batched = False\n",
    "        assert key.dim() == 2 and value.dim() == 2, (\n",
    "            \"For unbatched (2-D) `query`, expected `key` and `value` to be 2-D\"\n",
    "            f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\"\n",
    "        )\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.dim() == 1, (\n",
    "                \"For unbatched (2-D) `query`, expected `key_padding_mask` to be `None` or 1-D\"\n",
    "                f\" but found {key_padding_mask.dim()}-D tensor instead\"\n",
    "            )\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            assert attn_mask.dim() in (2, 3), (\n",
    "                \"For unbatched (2-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n",
    "                f\" but found {attn_mask.dim()}-D tensor instead\"\n",
    "            )\n",
    "            if attn_mask.dim() == 3:\n",
    "                expected_shape = (num_heads, query.shape[0], key.shape[0])\n",
    "                assert (\n",
    "                    attn_mask.shape == expected_shape\n",
    "                ), f\"Expected `attn_mask` shape to be {expected_shape} but got {attn_mask.shape}\"\n",
    "    else:\n",
    "        raise AssertionError(\n",
    "            f\"query should be unbatched 2D or batched 3D tensor but received {query.dim()}-D query tensor\"\n",
    "        )\n",
    "\n",
    "    return is_batched\n",
    "\n",
    "def set_fastpath_enabled(value: bool) -> None:\n",
    "    \"\"\"Sets whether fast path is enabled\"\"\"\n",
    "    global _is_fastpath_enabled\n",
    "    _is_fastpath_enabled = value\n",
    "\n",
    "\n",
    "def multi_head_attention_forward(\n",
    "    query: Tensor,\n",
    "    key: Tensor,\n",
    "    value: Tensor,\n",
    "    embed_dim_to_check: int,\n",
    "    num_heads: int,\n",
    "    in_proj_weight: Optional[Tensor],\n",
    "    in_proj_bias: Optional[Tensor],\n",
    "    bias_k: Optional[Tensor],\n",
    "    bias_v: Optional[Tensor],\n",
    "    add_zero_attn: bool,\n",
    "    dropout_p: float,\n",
    "    out_proj_weight: Tensor,\n",
    "    out_proj_bias: Optional[Tensor],\n",
    "    training: bool = True,\n",
    "    key_padding_mask: Optional[Tensor] = None,\n",
    "    need_weights: bool = True,\n",
    "    attn_mask: Optional[Tensor] = None,\n",
    "    use_separate_proj_weight: bool = False,\n",
    "    q_proj_weight: Optional[Tensor] = None,\n",
    "    k_proj_weight: Optional[Tensor] = None,\n",
    "    v_proj_weight: Optional[Tensor] = None,\n",
    "    static_k: Optional[Tensor] = None,\n",
    "    static_v: Optional[Tensor] = None,\n",
    "    average_attn_weights: bool = True,\n",
    "    is_causal: bool = False,\n",
    "    return_pre_softmax: bool = False,\n",
    ") -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    r\"\"\"Forward method for MultiHeadAttention.\n",
    "\n",
    "    See :class:`torch.nn.MultiheadAttention` for details.\n",
    "\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        embed_dim_to_check: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        dropout_p: probability of an element to be zeroed.\n",
    "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
    "        training: apply dropout if is ``True``.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "            Default: `True`\n",
    "            Note: `needs_weight` defaults to `True`, but should be set to `False`\n",
    "            For best performance when attention weights are not needed.\n",
    "            *Setting needs_weights to `True`\n",
    "            leads to a significant performance degradation.*\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "        is_causal: If specified, applies a causal mask as attention mask, and ignores\n",
    "            attn_mask for computing scaled dot product attention.\n",
    "            Default: ``False``.\n",
    "            .. warning::\n",
    "                is_causal is provides a hint that the attn_mask is the\n",
    "                causal mask.Providing incorrect hints can result in\n",
    "                incorrect execution, including forward and backward\n",
    "                compatibility.\n",
    "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
    "            and value in different forms. If false, in_proj_weight will be used, which is\n",
    "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
    "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        static_k, static_v: static key and value used for attention operators.\n",
    "        average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.\n",
    "            Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect\n",
    "            when ``need_weights=True.``. Default: True\n",
    "\n",
    "\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a FloatTensor is provided, it will be directly added to the value.\n",
    "          If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
    "          positions. If a BoolTensor is provided, positions with ``True``\n",
    "          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "\n",
    "        Outputs:\n",
    "        - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns\n",
    "          attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n",
    "          :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n",
    "          :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n",
    "          head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.\n",
    "    \"\"\"\n",
    "    tens_ops = (\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        in_proj_weight,\n",
    "        in_proj_bias,\n",
    "        bias_k,\n",
    "        bias_v,\n",
    "        out_proj_weight,\n",
    "        out_proj_bias,\n",
    "    )\n",
    "    if has_torch_function(tens_ops):\n",
    "        return handle_torch_function(\n",
    "            multi_head_attention_forward,\n",
    "            tens_ops,\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            embed_dim_to_check,\n",
    "            num_heads,\n",
    "            in_proj_weight,\n",
    "            in_proj_bias,\n",
    "            bias_k,\n",
    "            bias_v,\n",
    "            add_zero_attn,\n",
    "            dropout_p,\n",
    "            out_proj_weight,\n",
    "            out_proj_bias,\n",
    "            training=training,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=need_weights,\n",
    "            attn_mask=attn_mask,\n",
    "            is_causal=is_causal,\n",
    "            use_separate_proj_weight=use_separate_proj_weight,\n",
    "            q_proj_weight=q_proj_weight,\n",
    "            k_proj_weight=k_proj_weight,\n",
    "            v_proj_weight=v_proj_weight,\n",
    "            static_k=static_k,\n",
    "            static_v=static_v,\n",
    "            average_attn_weights=average_attn_weights,\n",
    "            return_pre_softmax=return_pre_softmax,\n",
    "        )\n",
    "\n",
    "    is_batched = _mha_shape_check(\n",
    "        query, key, value, key_padding_mask, attn_mask, num_heads\n",
    "    )\n",
    "\n",
    "    # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\n",
    "    # is batched, run the computation and before returning squeeze the\n",
    "    # batch dimension so that the output doesn't carry this temporary batch dimension.\n",
    "    if not is_batched:\n",
    "        # unsqueeze if the input is unbatched\n",
    "        query = query.unsqueeze(1)\n",
    "        key = key.unsqueeze(1)\n",
    "        value = value.unsqueeze(1)\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(0)\n",
    "\n",
    "    # set up shape vars\n",
    "    tgt_len, bsz, embed_dim = query.shape\n",
    "    src_len, _, _ = key.shape\n",
    "\n",
    "    key_padding_mask = _canonical_mask(\n",
    "        mask=key_padding_mask,\n",
    "        mask_name=\"key_padding_mask\",\n",
    "        other_type=_none_or_dtype(attn_mask),\n",
    "        other_name=\"attn_mask\",\n",
    "        target_type=query.dtype,\n",
    "    )\n",
    "\n",
    "    if is_causal and attn_mask is None:\n",
    "        raise RuntimeError(\n",
    "            \"Need attn_mask if specifying the is_causal hint. \"\n",
    "            \"You may use the Transformer module method \"\n",
    "            \"`generate_square_subsequent_mask` to create this mask.\"\n",
    "        )\n",
    "\n",
    "    if is_causal and key_padding_mask is None and not need_weights:\n",
    "        # when we have a kpm or need weights, we need attn_mask\n",
    "        # Otherwise, we use the is_causal hint go as is_causal\n",
    "        # indicator to SDPA.\n",
    "        attn_mask = None\n",
    "    else:\n",
    "        attn_mask = _canonical_mask(\n",
    "            mask=attn_mask,\n",
    "            mask_name=\"attn_mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=query.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            # We have the attn_mask, and use that to merge kpm into it.\n",
    "            # Turn off use of is_causal hint, as the merged mask is no\n",
    "            # longer causal.\n",
    "            is_causal = False\n",
    "\n",
    "    assert embed_dim == embed_dim_to_check, (\n",
    "        f\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\n",
    "    )\n",
    "    if isinstance(embed_dim, torch.Tensor):\n",
    "        # embed_dim can be a tensor when JIT tracing\n",
    "        head_dim = embed_dim.div(num_heads, rounding_mode=\"trunc\")\n",
    "    else:\n",
    "        head_dim = embed_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, (\n",
    "        f\"embed_dim {embed_dim} not divisible by num_heads {num_heads}\"\n",
    "    )\n",
    "    if use_separate_proj_weight:\n",
    "        # allow MHA to have different embedding dimensions when separate projection weights are used\n",
    "        assert key.shape[:2] == value.shape[:2], (\n",
    "            f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\"\n",
    "        )\n",
    "    else:\n",
    "        assert key.shape == value.shape, (\n",
    "            f\"key shape {key.shape} does not match value shape {value.shape}\"\n",
    "        )\n",
    "\n",
    "    #\n",
    "    # compute in-projection\n",
    "    #\n",
    "    if not use_separate_proj_weight:\n",
    "        assert in_proj_weight is not None, (\n",
    "            \"use_separate_proj_weight is False but in_proj_weight is None\"\n",
    "        )\n",
    "        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
    "    else:\n",
    "        assert q_proj_weight is not None, (\n",
    "            \"use_separate_proj_weight is True but q_proj_weight is None\"\n",
    "        )\n",
    "        assert k_proj_weight is not None, (\n",
    "            \"use_separate_proj_weight is True but k_proj_weight is None\"\n",
    "        )\n",
    "        assert v_proj_weight is not None, (\n",
    "            \"use_separate_proj_weight is True but v_proj_weight is None\"\n",
    "        )\n",
    "        if in_proj_bias is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = in_proj_bias.chunk(3)\n",
    "        q, k, v = _in_projection(\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            q_proj_weight,\n",
    "            k_proj_weight,\n",
    "            v_proj_weight,\n",
    "            b_q,\n",
    "            b_k,\n",
    "            b_v,\n",
    "        )\n",
    "\n",
    "    # prep attention mask\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        # ensure attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(\n",
    "                    f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\"\n",
    "                )\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(\n",
    "                    f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\"\n",
    "                )\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"attn_mask's dimension {attn_mask.dim()} is not supported\"\n",
    "            )\n",
    "\n",
    "    # add bias along batch dimension (currently second)\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        assert static_k is None, \"bias cannot be added to static key.\"\n",
    "        assert static_v is None, \"bias cannot be added to static value.\"\n",
    "        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    #\n",
    "    # reshape q, k, v for multihead attention and make them batch first\n",
    "    #\n",
    "    q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if static_k is None:\n",
    "        k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    else:\n",
    "        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
    "        assert static_k.size(0) == bsz * num_heads, (\n",
    "            f\"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}\"\n",
    "        )\n",
    "        assert static_k.size(2) == head_dim, (\n",
    "            f\"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}\"\n",
    "        )\n",
    "        k = static_k\n",
    "    if static_v is None:\n",
    "        v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    else:\n",
    "        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
    "        assert static_v.size(0) == bsz * num_heads, (\n",
    "            f\"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}\"\n",
    "        )\n",
    "        assert static_v.size(2) == head_dim, (\n",
    "            f\"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}\"\n",
    "        )\n",
    "        v = static_v\n",
    "\n",
    "    # add zero attention along batch dimension (now first)\n",
    "    if add_zero_attn:\n",
    "        zero_attn_shape = (bsz * num_heads, 1, head_dim)\n",
    "        k = torch.cat(\n",
    "            [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1\n",
    "        )\n",
    "        v = torch.cat(\n",
    "            [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1\n",
    "        )\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "\n",
    "    # update source sequence length after adjustments\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    # merge key padding and attention masks\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.shape == (\n",
    "            bsz,\n",
    "            src_len,\n",
    "        ), f\"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}\"\n",
    "        key_padding_mask = (\n",
    "            key_padding_mask.view(bsz, 1, 1, src_len)\n",
    "            .expand(-1, num_heads, -1, -1)\n",
    "            .reshape(bsz * num_heads, 1, src_len)\n",
    "        )\n",
    "        if attn_mask is None:\n",
    "            attn_mask = key_padding_mask\n",
    "        else:\n",
    "            attn_mask = attn_mask + key_padding_mask\n",
    "\n",
    "    # adjust dropout probability\n",
    "    if not training:\n",
    "        dropout_p = 0.0\n",
    "\n",
    "    #\n",
    "    # (deep breath) calculate attention and out projection\n",
    "    #\n",
    "\n",
    "    if need_weights:\n",
    "        _B, _Nt, E = q.shape\n",
    "        q_scaled = q * math.sqrt(1.0 / float(E))\n",
    "\n",
    "        assert not (is_causal and attn_mask is None), (\n",
    "            \"FIXME: is_causal not implemented for need_weights\"\n",
    "        )\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            pre_softmax_interaction = attn_mask\n",
    "            pre_softmax_interaction.detach()\n",
    "            pre_softmax_attention = torch.bmm(q_scaled, k.transpose(-2, -1))\n",
    "            pre_softmax_attention.detach()\n",
    "            attn_output_weights = torch.baddbmm(\n",
    "                attn_mask, q_scaled, k.transpose(-2, -1)\n",
    "            )\n",
    "        else:\n",
    "            attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))\n",
    "        \n",
    "        attn_output_weights = torch.softmax(attn_output_weights, dim=-1)\n",
    "        if dropout_p > 0.0:\n",
    "            attn_output_weights = torch.dropout(attn_output_weights, p=dropout_p, train=training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "\n",
    "        attn_output = (\n",
    "            attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "        )\n",
    "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        # optionally average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        if average_attn_weights:\n",
    "            attn_output_weights = attn_output_weights.mean(dim=1)\n",
    "\n",
    "        if not is_batched:\n",
    "            # squeeze the output if input was unbatched\n",
    "            attn_output = attn_output.squeeze(1)\n",
    "            attn_output_weights = attn_output_weights.squeeze(0)\n",
    "        if return_pre_softmax:\n",
    "            return attn_output, attn_output_weights, pre_softmax_attention, pre_softmax_interaction\n",
    "        else:\n",
    "            return attn_output, attn_output_weights\n",
    "    else:\n",
    "        # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "        # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "        # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "            else:\n",
    "                attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "        q = q.view(bsz, num_heads, tgt_len, head_dim)\n",
    "        k = k.view(bsz, num_heads, src_len, head_dim)\n",
    "        v = v.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "        attn_output = scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask, dropout_p, is_causal\n",
    "        )\n",
    "        attn_output = (\n",
    "            attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "        )\n",
    "\n",
    "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
    "        if not is_batched:\n",
    "            # squeeze the output if input was unbatched\n",
    "            attn_output = attn_output.squeeze(1)\n",
    "        return attn_output, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "37783ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Multihead to get pre-softmax\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information from different representation subspaces.\n",
    "\n",
    "    This MultiheadAttention layer implements the original architecture described\n",
    "    in the `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_ paper. The\n",
    "    intent of this layer is as a reference implementation for foundational understanding\n",
    "    and thus it contains only limited features relative to newer architectures.\n",
    "    Given the fast pace of innovation in transformer-like architectures, we recommend\n",
    "    exploring this `tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n",
    "    to build efficient layers from building blocks in core or using higher\n",
    "    level libraries from the `PyTorch Ecosystem <https://landscape.pytorch.org/>`_.\n",
    "\n",
    "    Multi-Head Attention is defined as:\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
    "\n",
    "    where :math:`\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
    "\n",
    "    ``nn.MultiheadAttention`` will use the optimized implementations of\n",
    "    ``scaled_dot_product_attention()`` when possible.\n",
    "\n",
    "    In addition to support for the new ``scaled_dot_product_attention()``\n",
    "    function, for speeding up Inference, MHA will use\n",
    "    fastpath inference with support for Nested Tensors, iff:\n",
    "\n",
    "    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n",
    "    - inputs are batched (3D) with ``batch_first==True``\n",
    "    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n",
    "    - training is disabled (using ``.eval()``)\n",
    "    - ``add_bias_kv`` is ``False``\n",
    "    - ``add_zero_attn`` is ``False``\n",
    "    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n",
    "    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n",
    "      nor ``attn_mask`` is passed\n",
    "    - autocast is disabled\n",
    "\n",
    "    If the optimized inference fastpath implementation is in use, a\n",
    "    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n",
    "    ``query``/``key``/``value`` to represent padding more efficiently than using a\n",
    "    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n",
    "    will be returned, and an additional speedup proportional to the fraction of the input\n",
    "    that is padding can be expected.\n",
    "\n",
    "    Args:\n",
    "        embed_dim: Total dimension of the model.\n",
    "        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
    "            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
    "        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n",
    "        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n",
    "        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n",
    "        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n",
    "            Default: ``False``.\n",
    "        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n",
    "        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> # xdoctest: +SKIP\n",
    "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n",
    "         https://arxiv.org/abs/2205.14135\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = [\"batch_first\"]\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        dropout=0.0,\n",
    "        bias=True,\n",
    "        add_bias_kv=False,\n",
    "        add_zero_attn=False,\n",
    "        kdim=None,\n",
    "        vdim=None,\n",
    "        batch_first=False,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        return_pre_softmax: bool = False,\n",
    "    ) -> None:\n",
    "        if embed_dim <= 0 or num_heads <= 0:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim and num_heads must be greater than 0,\"\n",
    "                f\" got embed_dim={embed_dim} and num_heads={num_heads} instead\"\n",
    "            )\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.return_pre_softmax = return_pre_softmax\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, (\n",
    "            \"embed_dim must be divisible by num_heads\"\n",
    "        )\n",
    "\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            self.q_proj_weight = Parameter(\n",
    "                torch.empty((embed_dim, embed_dim), **factory_kwargs)\n",
    "            )\n",
    "            self.k_proj_weight = Parameter(\n",
    "                torch.empty((embed_dim, self.kdim), **factory_kwargs)\n",
    "            )\n",
    "            self.v_proj_weight = Parameter(\n",
    "                torch.empty((embed_dim, self.vdim), **factory_kwargs)\n",
    "            )\n",
    "            self.register_parameter(\"in_proj_weight\", None)\n",
    "        else:\n",
    "            self.in_proj_weight = Parameter(\n",
    "                torch.empty((3 * embed_dim, embed_dim), **factory_kwargs)\n",
    "            )\n",
    "            self.register_parameter(\"q_proj_weight\", None)\n",
    "            self.register_parameter(\"k_proj_weight\", None)\n",
    "            self.register_parameter(\"v_proj_weight\", None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter(\"in_proj_bias\", None)\n",
    "        self.out_proj = nn.Linear(\n",
    "            embed_dim, embed_dim, bias=bias, **factory_kwargs\n",
    "        )\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
    "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self) -> None:\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.0)\n",
    "            constant_(self.out_proj.bias, 0.0)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if \"_qkv_same_embed_dim\" not in state:\n",
    "            state[\"_qkv_same_embed_dim\"] = True\n",
    "\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: Tensor,\n",
    "        key: Tensor,\n",
    "        value: Tensor,\n",
    "        key_padding_mask: Optional[Tensor] = None,\n",
    "        need_weights: bool = True,\n",
    "        attn_mask: Optional[Tensor] = None,\n",
    "        average_attn_weights: bool = True,\n",
    "        is_causal: bool = False,\n",
    "        return_pre_softmax: bool = False,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        r\"\"\"Compute attention outputs using query, key, and value embeddings.\n",
    "\n",
    "            Supports optional parameters for padding, masks and attention weights.\n",
    "\n",
    "        Args:\n",
    "            query: Query embeddings of shape :math:`(L, E_q)` for unbatched input, :math:`(L, N, E_q)` when ``batch_first=False``\n",
    "                or :math:`(N, L, E_q)` when ``batch_first=True``, where :math:`L` is the target sequence length,\n",
    "                :math:`N` is the batch size, and :math:`E_q` is the query embedding dimension ``embed_dim``.\n",
    "                Queries are compared against key-value pairs to produce the output.\n",
    "                See \"Attention Is All You Need\" for more details.\n",
    "            key: Key embeddings of shape :math:`(S, E_k)` for unbatched input, :math:`(S, N, E_k)` when ``batch_first=False``\n",
    "                or :math:`(N, S, E_k)` when ``batch_first=True``, where :math:`S` is the source sequence length,\n",
    "                :math:`N` is the batch size, and :math:`E_k` is the key embedding dimension ``kdim``.\n",
    "                See \"Attention Is All You Need\" for more details.\n",
    "            value: Value embeddings of shape :math:`(S, E_v)` for unbatched input, :math:`(S, N, E_v)` when\n",
    "                ``batch_first=False`` or :math:`(N, S, E_v)` when ``batch_first=True``, where :math:`S` is the source\n",
    "                sequence length, :math:`N` is the batch size, and :math:`E_v` is the value embedding dimension ``vdim``.\n",
    "                See \"Attention Is All You Need\" for more details.\n",
    "            key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``\n",
    "                to ignore for the purpose of attention (i.e. treat as \"padding\"). For unbatched `query`, shape should be :math:`(S)`.\n",
    "                Binary and float masks are supported.\n",
    "                For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for\n",
    "                the purpose of attention. For a float mask, it will be directly added to the corresponding ``key`` value.\n",
    "            need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.\n",
    "                Set ``need_weights=False`` to use the optimized ``scaled_dot_product_attention``\n",
    "                and achieve the best performance for MHA.\n",
    "                Default: ``True``.\n",
    "            attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\n",
    "                :math:`(L, S)` or :math:`(N\\cdot\\text{num\\_heads}, L, S)`, where :math:`N` is the batch size,\n",
    "                :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be\n",
    "                broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\n",
    "                Binary and float masks are supported. For a binary mask, a ``True`` value indicates that the\n",
    "                corresponding position is not allowed to attend. For a float mask, the mask values will be added to\n",
    "                the attention weight.\n",
    "                If both attn_mask and key_padding_mask are supplied, their types should match.\n",
    "            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n",
    "                heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n",
    "                effect when ``need_weights=True``. Default: ``True`` (i.e. average weights across heads)\n",
    "            is_causal: If specified, applies a causal mask as attention mask.\n",
    "                Default: ``False``.\n",
    "                Warning:\n",
    "                ``is_causal`` provides a hint that ``attn_mask`` is the\n",
    "                causal mask. Providing incorrect hints can result in\n",
    "                incorrect execution, including forward and backward\n",
    "                compatibility.\n",
    "\n",
    "        Outputs:\n",
    "            - **attn_output** - Attention outputs of shape :math:`(L, E)` when input is unbatched,\n",
    "              :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` when ``batch_first=True``,\n",
    "              where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` is the\n",
    "              embedding dimension ``embed_dim``.\n",
    "            - **attn_output_weights** - Only returned when ``need_weights=True``. If ``average_attn_weights=True``,\n",
    "              returns attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n",
    "              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n",
    "              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n",
    "              head of shape :math:`(\\text{num\\_heads}, L, S)` when input is unbatched or :math:`(N, \\text{num\\_heads}, L, S)`.\n",
    "\n",
    "            .. note::\n",
    "                `batch_first` argument is ignored for unbatched inputs.\n",
    "        \"\"\"  # noqa: B950\n",
    "        why_not_fast_path = \"\"\n",
    "        if (\n",
    "            (attn_mask is not None and torch.is_floating_point(attn_mask))\n",
    "            or (key_padding_mask is not None)\n",
    "            and torch.is_floating_point(key_padding_mask)\n",
    "        ):\n",
    "            why_not_fast_path = \"floating-point masks are not supported for fast path.\"\n",
    "\n",
    "        is_batched = query.dim() == 3\n",
    "\n",
    "        key_padding_mask = _canonical_mask(\n",
    "            mask=key_padding_mask,\n",
    "            mask_name=\"key_padding_mask\",\n",
    "            other_type=_none_or_dtype(attn_mask),\n",
    "            other_name=\"attn_mask\",\n",
    "            target_type=query.dtype,\n",
    "        )\n",
    "\n",
    "        attn_mask = _canonical_mask(\n",
    "            mask=attn_mask,\n",
    "            mask_name=\"attn_mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=query.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "        is_fastpath_enabled = get_fastpath_enabled()\n",
    "\n",
    "        if not is_fastpath_enabled:\n",
    "            why_not_fast_path = \"get_fastpath_enabled() was not True\"\n",
    "        elif not is_batched:\n",
    "            why_not_fast_path = (\n",
    "                f\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\n",
    "            )\n",
    "        elif query is not key or key is not value:\n",
    "            # When lifting this restriction, don't forget to either\n",
    "            # enforce that the dtypes all match or test cases where\n",
    "            # they don't!\n",
    "            why_not_fast_path = \"non-self attention was used (query, key, and value are not the same Tensor)\"\n",
    "        elif self.in_proj_bias is not None and query.dtype != self.in_proj_bias.dtype:\n",
    "            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\n",
    "        elif self.in_proj_weight is None:\n",
    "            why_not_fast_path = \"in_proj_weight was None\"\n",
    "        elif query.dtype != self.in_proj_weight.dtype:\n",
    "            # this case will fail anyway, but at least they'll get a useful error message.\n",
    "            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n",
    "        elif self.training:\n",
    "            why_not_fast_path = \"training is enabled\"\n",
    "        elif (self.num_heads % 2) != 0:\n",
    "            why_not_fast_path = \"self.num_heads is not even\"\n",
    "        elif not self.batch_first:\n",
    "            why_not_fast_path = \"batch_first was not True\"\n",
    "        elif self.bias_k is not None:\n",
    "            why_not_fast_path = \"self.bias_k was not None\"\n",
    "        elif self.bias_v is not None:\n",
    "            why_not_fast_path = \"self.bias_v was not None\"\n",
    "        elif self.add_zero_attn:\n",
    "            why_not_fast_path = \"add_zero_attn was enabled\"\n",
    "        elif not self._qkv_same_embed_dim:\n",
    "            why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n",
    "        elif query.is_nested and (\n",
    "            key_padding_mask is not None or attn_mask is not None\n",
    "        ):\n",
    "            why_not_fast_path = (\n",
    "                \"supplying both src_key_padding_mask and src_mask at the same time \\\n",
    "                                 is not supported with NestedTensor input\"\n",
    "            )\n",
    "        elif torch.is_autocast_enabled():\n",
    "            why_not_fast_path = \"autocast is enabled\"\n",
    "\n",
    "        if not why_not_fast_path:\n",
    "            tensor_args = (\n",
    "                query,\n",
    "                key,\n",
    "                value,\n",
    "                self.in_proj_weight,\n",
    "                self.in_proj_bias,\n",
    "                self.out_proj.weight,\n",
    "                self.out_proj.bias,\n",
    "            )\n",
    "            # We have to use list comprehensions below because TorchScript does not support\n",
    "            # generator expressions.\n",
    "            if torch.overrides.has_torch_function(tensor_args):\n",
    "                why_not_fast_path = \"some Tensor argument has_torch_function\"\n",
    "            elif _is_make_fx_tracing():\n",
    "                why_not_fast_path = \"we are running make_fx tracing\"\n",
    "            elif not all(_check_arg_device(x) for x in tensor_args):\n",
    "                why_not_fast_path = (\n",
    "                    \"some Tensor argument's device is neither one of \"\n",
    "                    f\"cpu, cuda or {torch.utils.backend_registration._privateuse1_backend_name}\"\n",
    "                )\n",
    "            elif torch.is_grad_enabled() and any(\n",
    "                _arg_requires_grad(x) for x in tensor_args\n",
    "            ):\n",
    "                why_not_fast_path = (\n",
    "                    \"grad is enabled and at least one of query or the \"\n",
    "                    \"input/output projection weights or biases requires_grad\"\n",
    "                )\n",
    "            if not why_not_fast_path:\n",
    "                merged_mask, mask_type = self.merge_masks(\n",
    "                    attn_mask, key_padding_mask, query\n",
    "                )\n",
    "\n",
    "                if self.in_proj_bias is not None and self.in_proj_weight is not None:\n",
    "                    return torch._native_multi_head_attention(\n",
    "                        query,\n",
    "                        key,\n",
    "                        value,\n",
    "                        self.embed_dim,\n",
    "                        self.num_heads,\n",
    "                        self.in_proj_weight,\n",
    "                        self.in_proj_bias,\n",
    "                        self.out_proj.weight,\n",
    "                        self.out_proj.bias,\n",
    "                        merged_mask,\n",
    "                        need_weights,\n",
    "                        average_attn_weights,\n",
    "                        mask_type,\n",
    "                    )\n",
    "\n",
    "        any_nested = query.is_nested or key.is_nested or value.is_nested\n",
    "        assert not any_nested, (\n",
    "            \"MultiheadAttention does not support NestedTensor outside of its fast path. \"\n",
    "            + f\"The fast path was not hit because {why_not_fast_path}\"\n",
    "        )\n",
    "\n",
    "        if self.batch_first and is_batched:\n",
    "            # make sure that the transpose op does not affect the \"is\" property\n",
    "            if key is value:\n",
    "                if query is key:\n",
    "                    query = key = value = query.transpose(1, 0)\n",
    "                else:\n",
    "                    query, key = (x.transpose(1, 0) for x in (query, key))\n",
    "                    value = key\n",
    "            else:\n",
    "                query, key, value = (x.transpose(1, 0) for x in (query, key, value))\n",
    "\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            if self.return_pre_softmax:\n",
    "                attn_output, attn_output_weights, pre_softmax_attention, pre_softmax_interaction = multi_head_attention_forward(\n",
    "                    query, key, value, self.embed_dim, self.num_heads, self.in_proj_weight, self.in_proj_bias, self.bias_k, self.bias_v,\n",
    "                self.add_zero_attn, self.dropout, self.out_proj.weight, self.out_proj.bias, training=self.training, key_padding_mask=key_padding_mask,\n",
    "                need_weights=need_weights, attn_mask=attn_mask, use_separate_proj_weight=True, q_proj_weight=self.q_proj_weight,\n",
    "                k_proj_weight=self.k_proj_weight, v_proj_weight=self.v_proj_weight, average_attn_weights=average_attn_weights, is_causal=is_causal,\n",
    "                return_pre_softmax=self.return_pre_softmax,\n",
    "            )\n",
    "                pre_softmax_attention.cpu().detach()\n",
    "                pre_softmax_interaction.cpu().detach()\n",
    "            else:\n",
    "                attn_output, attn_output_weights = multi_head_attention_forward(\n",
    "                    query, key, value, self.embed_dim, self.num_heads, self.in_proj_weight, self.in_proj_bias, self.bias_k, self.bias_v,\n",
    "                self.add_zero_attn, self.dropout, self.out_proj.weight, self.out_proj.bias, training=self.training, key_padding_mask=key_padding_mask,\n",
    "                need_weights=need_weights, attn_mask=attn_mask, use_separate_proj_weight=True, q_proj_weight=self.q_proj_weight,\n",
    "                k_proj_weight=self.k_proj_weight, v_proj_weight=self.v_proj_weight, average_attn_weights=average_attn_weights, is_causal=is_causal,\n",
    "                return_pre_softmax=self.return_pre_softmax,\n",
    "            )\n",
    "        else:\n",
    "            if self.return_pre_softmax:\n",
    "                attn_output, attn_output_weights, pre_softmax_attention, pre_softmax_interaction = multi_head_attention_forward(\n",
    "                    query,\n",
    "                    key,\n",
    "                    value,\n",
    "                    self.embed_dim,\n",
    "                    self.num_heads,\n",
    "                    self.in_proj_weight,\n",
    "                    self.in_proj_bias,\n",
    "                    self.bias_k,\n",
    "                    self.bias_v,\n",
    "                    self.add_zero_attn,\n",
    "                    self.dropout,\n",
    "                    self.out_proj.weight,\n",
    "                    self.out_proj.bias,\n",
    "                    training=self.training,\n",
    "                    key_padding_mask=key_padding_mask,\n",
    "                    need_weights=need_weights,\n",
    "                    attn_mask=attn_mask,\n",
    "                    average_attn_weights=average_attn_weights,\n",
    "                    is_causal=is_causal,\n",
    "                    return_pre_softmax=self.return_pre_softmax,\n",
    "                )\n",
    "                pre_softmax_attention.cpu().detach()\n",
    "                pre_softmax_interaction.cpu().detach()\n",
    "            else:\n",
    "                attn_output, attn_output_weights = multi_head_attention_forward(\n",
    "                query,\n",
    "                key,\n",
    "                value,\n",
    "                self.embed_dim,\n",
    "                self.num_heads,\n",
    "                self.in_proj_weight,\n",
    "                self.in_proj_bias,\n",
    "                self.bias_k,\n",
    "                self.bias_v,\n",
    "                self.add_zero_attn,\n",
    "                self.dropout,\n",
    "                self.out_proj.weight,\n",
    "                self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask,\n",
    "                need_weights=need_weights,\n",
    "                attn_mask=attn_mask,\n",
    "                average_attn_weights=average_attn_weights,\n",
    "                is_causal=is_causal,\n",
    "                return_pre_softmax=self.return_pre_softmax,\n",
    "            )\n",
    "        if self.batch_first and is_batched:\n",
    "            if self.return_pre_softmax:\n",
    "                pre_softmax_attention.cpu().detach()\n",
    "                pre_softmax_interaction.cpu().detach()\n",
    "                return attn_output.transpose(1, 0), attn_output_weights, pre_softmax_attention, pre_softmax_interaction\n",
    "            else:\n",
    "                return attn_output.transpose(1, 0), attn_output_weights\n",
    "        else:\n",
    "            if self.return_pre_softmax:\n",
    "                    pre_softmax_attention.cpu().detach()\n",
    "                    pre_softmax_interaction.cpu().detach()\n",
    "                    return attn_output, attn_output_weights, pre_softmax_attention, pre_softmax_interaction\n",
    "            else:\n",
    "                return attn_output, attn_output_weights\n",
    "\n",
    "    def merge_masks(\n",
    "        self,\n",
    "        attn_mask: Optional[Tensor],\n",
    "        key_padding_mask: Optional[Tensor],\n",
    "        query: Tensor,\n",
    "    ) -> Tuple[Optional[Tensor], Optional[int]]:\n",
    "        r\"\"\"Determine mask type and combine masks if necessary.\n",
    "\n",
    "        If only one mask is provided, that mask\n",
    "        and the corresponding mask type will be returned. If both masks are provided, they will be both\n",
    "        expanded to shape ``(batch_size, num_heads, seq_len, seq_len)``, combined with logical ``or``\n",
    "        and mask type 2 will be returned\n",
    "        Args:\n",
    "            attn_mask: attention mask of shape ``(seq_len, seq_len)``, mask type 0\n",
    "            key_padding_mask: padding mask of shape ``(batch_size, seq_len)``, mask type 1\n",
    "            query: query embeddings of shape ``(batch_size, seq_len, embed_dim)``\n",
    "        Returns:\n",
    "            merged_mask: merged mask\n",
    "            mask_type: merged mask type (0, 1, or 2)\n",
    "        \"\"\"\n",
    "        mask_type: Optional[int] = None\n",
    "        merged_mask: Optional[Tensor] = None\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            mask_type = 1\n",
    "            merged_mask = key_padding_mask\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            # In this branch query can't be a nested tensor, so it has a shape\n",
    "            batch_size, seq_len, _ = query.shape\n",
    "            mask_type = 2\n",
    "\n",
    "            # Always expands attn_mask to 4D\n",
    "            if attn_mask.dim() == 3:\n",
    "                attn_mask_expanded = attn_mask.view(batch_size, -1, seq_len, seq_len)\n",
    "            else:  # attn_mask.dim() == 2:\n",
    "                attn_mask_expanded = attn_mask.view(1, 1, seq_len, seq_len).expand(\n",
    "                    batch_size, self.num_heads, -1, -1\n",
    "                )\n",
    "            merged_mask = attn_mask_expanded\n",
    "\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask_expanded = key_padding_mask.view(\n",
    "                    batch_size, 1, 1, seq_len\n",
    "                ).expand(-1, self.num_heads, -1, -1)\n",
    "                merged_mask = attn_mask_expanded + key_padding_mask_expanded\n",
    "\n",
    "        # no attn_mask and no key_padding_mask, returns None, None\n",
    "        return merged_mask, mask_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af75fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "581ff85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "@torch.jit.script\n",
    "def delta_phi(a, b):\n",
    "    return (a - b + math.pi) % (2 * math.pi) - math.pi\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def delta_r2(eta1, phi1, eta2, phi2):\n",
    "    return (eta1 - eta2)**2 + delta_phi(phi1, phi2)**2\n",
    "\n",
    "\n",
    "def to_pt2(x, eps=1e-8):\n",
    "    pt2 = x[:, :2].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        pt2 = pt2.clamp(min=eps)\n",
    "    return pt2\n",
    "\n",
    "\n",
    "def to_m2(x, eps=1e-8):\n",
    "    m2 = x[:, 3:4].square() - x[:, :3].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        m2 = m2.clamp(min=eps)\n",
    "    return m2\n",
    "\n",
    "\n",
    "def atan2(y, x):\n",
    "    sx = torch.sign(x)\n",
    "    sy = torch.sign(y)\n",
    "    pi_part = (sy + sx * (sy ** 2 - 1)) * (sx - 1) * (-math.pi / 2)\n",
    "    atan_part = torch.arctan(y / (x + (1 - sx ** 2))) * sx ** 2\n",
    "    return atan_part + pi_part\n",
    "\n",
    "\n",
    "def to_ptrapphim(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
    "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
    "    px, py, pz, energy = x.split((1, 1, 1, 1), dim=1)\n",
    "    pt = torch.sqrt(to_pt2(x, eps=eps))\n",
    "    # rapidity = 0.5 * torch.log((energy + pz) / (energy - pz))\n",
    "    rapidity = 0.5 * torch.log(1 + (2 * pz) / (energy - pz).clamp(min=1e-20))\n",
    "    phi = (atan2 if for_onnx else torch.atan2)(py, px)\n",
    "    if not return_mass:\n",
    "        return torch.cat((pt, rapidity, phi), dim=1)\n",
    "    else:\n",
    "        m = torch.sqrt(to_m2(x, eps=eps))\n",
    "        return torch.cat((pt, rapidity, phi, m), dim=1)\n",
    "\n",
    "\n",
    "def boost(x, boostp4, eps=1e-8):\n",
    "    # boost x to the rest frame of boostp4\n",
    "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
    "    p3 = -boostp4[:, :3] / boostp4[:, 3:].clamp(min=eps)\n",
    "    b2 = p3.square().sum(dim=1, keepdim=True)\n",
    "    gamma = (1 - b2).clamp(min=eps)**(-0.5)\n",
    "    gamma2 = (gamma - 1) / b2\n",
    "    gamma2.masked_fill_(b2 == 0, 0)\n",
    "    bp = (x[:, :3] * p3).sum(dim=1, keepdim=True)\n",
    "    v = x[:, :3] + gamma2 * bp * p3 + x[:, 3:] * gamma * p3\n",
    "    return v\n",
    "\n",
    "\n",
    "def p3_norm(p, eps=1e-8):\n",
    "    return p[:, :3] / p[:, :3].norm(dim=1, keepdim=True).clamp(min=eps)\n",
    "\n",
    "\n",
    "def pairwise_lv_fts(xi, xj, num_outputs=4, eps=1e-8, for_onnx=False):\n",
    "    pti, rapi, phii = to_ptrapphim(xi, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
    "    ptj, rapj, phij = to_ptrapphim(xj, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
    "\n",
    "    delta = delta_r2(rapi, phii, rapj, phij).sqrt()\n",
    "    lndelta = torch.log(delta.clamp(min=eps))\n",
    "    if num_outputs == 1:\n",
    "        return lndelta\n",
    "\n",
    "    if num_outputs > 1:\n",
    "        ptmin = ((pti <= ptj) * pti + (pti > ptj) * ptj) if for_onnx else torch.minimum(pti, ptj)\n",
    "        lnkt = torch.log((ptmin * delta).clamp(min=eps))\n",
    "        lnz = torch.log((ptmin / (pti + ptj).clamp(min=eps)).clamp(min=eps))\n",
    "        outputs = [lnkt, lnz, lndelta]\n",
    "\n",
    "    if num_outputs > 3:\n",
    "        xij = xi + xj\n",
    "        lnm2 = torch.log(to_m2(xij, eps=eps))\n",
    "        outputs.append(lnm2)\n",
    "\n",
    "    if num_outputs > 4:\n",
    "        lnds2 = torch.log(torch.clamp(-to_m2(xi - xj, eps=None), min=eps))\n",
    "        outputs.append(lnds2)\n",
    "\n",
    "    # the following features are not symmetric for (i, j)\n",
    "    if num_outputs > 5:\n",
    "        xj_boost = boost(xj, xij)\n",
    "        costheta = (p3_norm(xj_boost, eps=eps) * p3_norm(xij, eps=eps)).sum(dim=1, keepdim=True)\n",
    "        outputs.append(costheta)\n",
    "\n",
    "    if num_outputs > 6:\n",
    "        deltarap = rapi - rapj\n",
    "        deltaphi = delta_phi(phii, phij)\n",
    "        outputs += [deltarap, deltaphi]\n",
    "\n",
    "    assert (len(outputs) == num_outputs)\n",
    "    return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "def build_sparse_tensor(uu, idx, seq_len):\n",
    "    # inputs: uu (N, C, num_pairs), idx (N, 2, num_pairs)\n",
    "    # return: (N, C, seq_len, seq_len)\n",
    "    batch_size, num_fts, num_pairs = uu.size()\n",
    "    idx = torch.min(idx, torch.ones_like(idx) * seq_len)\n",
    "    i = torch.cat((\n",
    "        torch.arange(0, batch_size, device=uu.device).repeat_interleave(num_fts * num_pairs).unsqueeze(0),\n",
    "        torch.arange(0, num_fts, device=uu.device).repeat_interleave(num_pairs).repeat(batch_size).unsqueeze(0),\n",
    "        idx[:, :1, :].expand_as(uu).flatten().unsqueeze(0),\n",
    "        idx[:, 1:, :].expand_as(uu).flatten().unsqueeze(0),\n",
    "    ), dim=0)\n",
    "    return torch.sparse_coo_tensor(\n",
    "        i, uu.flatten(),\n",
    "        size=(batch_size, num_fts, seq_len + 1, seq_len + 1),\n",
    "        device=uu.device).to_dense()[:, :, :seq_len, :seq_len]\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # From https://github.com/rwightman/pytorch-image-models/blob/18ec173f95aa220af753358bf860b16b6691edb2/timm/layers/weight_init.py#L8\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class SequenceTrimmer(nn.Module):\n",
    "\n",
    "    def __init__(self, enabled=False, target=(0.9, 1.02), **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.enabled = enabled\n",
    "        self.target = target\n",
    "        self._counter = 0\n",
    "\n",
    "    def forward(self, x, v=None, mask=None, uu=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "        # uu: (N, C', P, P)\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(x[:, :1])\n",
    "        mask = mask.bool()\n",
    "\n",
    "        if self.enabled:\n",
    "            if self._counter < 5:\n",
    "                self._counter += 1\n",
    "            else:\n",
    "                if self.training:\n",
    "                    q = min(1, random.uniform(*self.target))\n",
    "                    maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
    "                    rand = torch.rand_like(mask.type_as(x))\n",
    "                    rand.masked_fill_(~mask, -1)\n",
    "                    perm = rand.argsort(dim=-1, descending=True)  # (N, 1, P)\n",
    "                    mask = torch.gather(mask, -1, perm)\n",
    "                    x = torch.gather(x, -1, perm.expand_as(x))\n",
    "                    if v is not None:\n",
    "                        v = torch.gather(v, -1, perm.expand_as(v))\n",
    "                    if uu is not None:\n",
    "                        uu = torch.gather(uu, -2, perm.unsqueeze(-1).expand_as(uu))\n",
    "                        uu = torch.gather(uu, -1, perm.unsqueeze(-2).expand_as(uu))\n",
    "                else:\n",
    "                    maxlen = mask.sum(dim=-1).max()\n",
    "                maxlen = max(maxlen, 1)\n",
    "                if maxlen < mask.size(-1):\n",
    "                    mask = mask[:, :, :maxlen]\n",
    "                    x = x[:, :, :maxlen]\n",
    "                    if v is not None:\n",
    "                        v = v[:, :, :maxlen]\n",
    "                    if uu is not None:\n",
    "                        uu = uu[:, :, :maxlen, :maxlen]\n",
    "\n",
    "        return x, v, mask, uu\n",
    "\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_bn = nn.BatchNorm1d(input_dim) if normalize_input else None\n",
    "        module_list = []\n",
    "        for dim in dims:\n",
    "            module_list.extend([\n",
    "                nn.LayerNorm(input_dim),\n",
    "                nn.Linear(input_dim, dim),\n",
    "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "            ])\n",
    "            input_dim = dim\n",
    "        self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.input_bn is not None:\n",
    "            # x: (batch, embed_dim, seq_len)\n",
    "            x = self.input_bn(x)\n",
    "            x = x.permute(2, 0, 1).contiguous()\n",
    "        # x: (seq_len, batch, embed_dim)\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class PairEmbed(nn.Module):\n",
    "    def __init__(\n",
    "            self, pairwise_lv_dim, pairwise_input_dim, dims,\n",
    "            remove_self_pair=False, use_pre_activation_pair=True, mode='sum',\n",
    "            normalize_input=True, activation='gelu', eps=1e-8,\n",
    "            for_onnx=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pairwise_lv_dim = pairwise_lv_dim\n",
    "        self.pairwise_input_dim = pairwise_input_dim\n",
    "        self.is_symmetric = (pairwise_lv_dim <= 5) and (pairwise_input_dim == 0)\n",
    "        self.remove_self_pair = remove_self_pair\n",
    "        self.mode = mode\n",
    "        self.for_onnx = for_onnx\n",
    "        self.pairwise_lv_fts = partial(pairwise_lv_fts, num_outputs=pairwise_lv_dim, eps=eps, for_onnx=for_onnx)\n",
    "        self.out_dim = dims[-1]\n",
    "\n",
    "        if self.mode == 'concat':\n",
    "            input_dim = pairwise_lv_dim + pairwise_input_dim\n",
    "            module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "            for dim in dims:\n",
    "                module_list.extend([\n",
    "                    nn.Conv1d(input_dim, dim, 1),\n",
    "                    nn.BatchNorm1d(dim),\n",
    "                    nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                ])\n",
    "                input_dim = dim\n",
    "            if use_pre_activation_pair:\n",
    "                module_list = module_list[:-1]\n",
    "            self.embed = nn.Sequential(*module_list)\n",
    "        elif self.mode == 'sum':\n",
    "            if pairwise_lv_dim > 0:\n",
    "                input_dim = pairwise_lv_dim\n",
    "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "                for dim in dims:\n",
    "                    module_list.extend([\n",
    "                        nn.Conv1d(input_dim, dim, 1),\n",
    "                        nn.BatchNorm1d(dim),\n",
    "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                    ])\n",
    "                    input_dim = dim\n",
    "                if use_pre_activation_pair:\n",
    "                    module_list = module_list[:-1]\n",
    "                self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "            if pairwise_input_dim > 0:\n",
    "                input_dim = pairwise_input_dim\n",
    "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "                for dim in dims:\n",
    "                    module_list.extend([\n",
    "                        nn.Conv1d(input_dim, dim, 1),\n",
    "                        nn.BatchNorm1d(dim),\n",
    "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                    ])\n",
    "                    input_dim = dim\n",
    "                if use_pre_activation_pair:\n",
    "                    module_list = module_list[:-1]\n",
    "                self.fts_embed = nn.Sequential(*module_list)\n",
    "        else:\n",
    "            raise RuntimeError('`mode` can only be `sum` or `concat`')\n",
    "\n",
    "    def forward(self, x, uu=None):\n",
    "        # x: (batch, v_dim, seq_len)\n",
    "        # uu: (batch, v_dim, seq_len, seq_len)\n",
    "        assert (x is not None or uu is not None)\n",
    "        with torch.no_grad():\n",
    "            if x is not None:\n",
    "                batch_size, _, seq_len = x.size()\n",
    "            else:\n",
    "                batch_size, _, seq_len, _ = uu.size()\n",
    "            if self.is_symmetric and not self.for_onnx:\n",
    "                i, j = torch.tril_indices(seq_len, seq_len, offset=-1 if self.remove_self_pair else 0,\n",
    "                                          device=(x if x is not None else uu).device)\n",
    "                if x is not None:\n",
    "                    x = x.unsqueeze(-1).repeat(1, 1, 1, seq_len)\n",
    "                    xi = x[:, :, i, j]  # (batch, dim, seq_len*(seq_len+1)/2)\n",
    "                    xj = x[:, :, j, i]\n",
    "                    x = self.pairwise_lv_fts(xi, xj)\n",
    "                if uu is not None:\n",
    "                    # (batch, dim, seq_len*(seq_len+1)/2)\n",
    "                    uu = uu[:, :, i, j]\n",
    "            else:\n",
    "                if x is not None:\n",
    "                    x = self.pairwise_lv_fts(x.unsqueeze(-1), x.unsqueeze(-2))\n",
    "                    if self.remove_self_pair:\n",
    "                        i = torch.arange(0, seq_len, device=x.device)\n",
    "                        x[:, :, i, i] = 0\n",
    "                    x = x.view(-1, self.pairwise_lv_dim, seq_len * seq_len)\n",
    "                if uu is not None:\n",
    "                    uu = uu.view(-1, self.pairwise_input_dim, seq_len * seq_len)\n",
    "            if self.mode == 'concat':\n",
    "                if x is None:\n",
    "                    pair_fts = uu\n",
    "                elif uu is None:\n",
    "                    pair_fts = x\n",
    "                else:\n",
    "                    pair_fts = torch.cat((x, uu), dim=1)\n",
    "\n",
    "        if self.mode == 'concat':\n",
    "            elements = self.embed(pair_fts)  # (batch, embed_dim, num_elements)\n",
    "        elif self.mode == 'sum':\n",
    "            if x is None:\n",
    "                elements = self.fts_embed(uu)\n",
    "            elif uu is None:\n",
    "                elements = self.embed(x)\n",
    "            else:\n",
    "                elements = self.embed(x) + self.fts_embed(uu)\n",
    "\n",
    "        if self.is_symmetric and not self.for_onnx:\n",
    "            y = torch.zeros(batch_size, self.out_dim, seq_len, seq_len, dtype=elements.dtype, device=elements.device)\n",
    "            y[:, :, i, j] = elements\n",
    "            y[:, :, j, i] = elements\n",
    "        else:\n",
    "            y = elements.view(-1, self.out_dim, seq_len, seq_len)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=8, ffn_ratio=4,\n",
    "                 dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                 add_bias_kv=False, activation='gelu',\n",
    "                 scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True,\n",
    "                 moe_num_experts=4, moe_top_k=1,\n",
    "                 moe_capacity_factor=1.25, moe_aux_loss_coef=0.01, moe_router_jitter=0.0,\n",
    "                 return_pre_softmax=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.ffn_dim = embed_dim * ffn_ratio\n",
    "        self.return_pre_softmax = return_pre_softmax\n",
    "\n",
    "        self.pre_attn_norm = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            dropout=attn_dropout,\n",
    "            add_bias_kv=add_bias_kv,\n",
    "            return_pre_softmax=return_pre_softmax,\n",
    "        )\n",
    "        self.post_attn_norm = nn.LayerNorm(embed_dim) if scale_attn else None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.pre_fc_norm = nn.LayerNorm(embed_dim)\n",
    "        self.act = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
    "        self.act_dropout = nn.Dropout(activation_dropout)\n",
    "        self.post_fc_norm = nn.LayerNorm(self.ffn_dim) if scale_fc else None\n",
    "\n",
    "        if not (moe_num_experts and moe_num_experts > 0):\n",
    "            raise ValueError('moe_num_experts must be >= 1 for MoE-only Block')\n",
    "        self.moe_num_experts = moe_num_experts\n",
    "        self.moe_top_k = moe_top_k\n",
    "        self.moe_capacity_factor = moe_capacity_factor\n",
    "        self.moe_aux_loss_coef = moe_aux_loss_coef\n",
    "        self.moe_router_jitter = moe_router_jitter\n",
    "        self.router = nn.Linear(embed_dim, moe_num_experts)\n",
    "        experts = []\n",
    "        for _ in range(moe_num_experts):\n",
    "            experts.append(nn.Sequential(\n",
    "                nn.Linear(embed_dim, self.ffn_dim),\n",
    "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                nn.Dropout(activation_dropout),\n",
    "                nn.LayerNorm(self.ffn_dim) if scale_fc else nn.Identity(),\n",
    "                nn.Linear(self.ffn_dim, embed_dim),\n",
    "            ))\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.register_buffer('_last_aux_loss', torch.tensor(0.0), persistent=False)\n",
    "\n",
    "        self.c_attn = nn.Parameter(torch.ones(num_heads), requires_grad=True) if scale_heads else None\n",
    "        self.w_resid = nn.Parameter(torch.ones(embed_dim), requires_grad=True) if scale_resids else None\n",
    "\n",
    "    def forward(self, x, x_cls=None, padding_mask=None, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            x_cls (Tensor, optional): class token input to the layer of shape `(1, batch, embed_dim)`\n",
    "            padding_mask (ByteTensor, optional): binary\n",
    "                ByteTensor of shape `(batch, seq_len)` where padding\n",
    "                elements are indicated by ``1``.\n",
    "\n",
    "        Returns:\n",
    "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
    "        \"\"\"\n",
    "\n",
    "        if x_cls is not None:\n",
    "            with torch.no_grad():\n",
    "                # prepend one element for x_cls: -> (batch, 1+seq_len)\n",
    "                padding_mask = torch.cat((torch.zeros_like(padding_mask[:, :1]), padding_mask), dim=1)\n",
    "            # class attention: https://arxiv.org/pdf/2103.17239.pdf\n",
    "            residual = x_cls\n",
    "            u = torch.cat((x_cls, x), dim=0)  # (seq_len+1, batch, embed_dim)\n",
    "            u = self.pre_attn_norm(u)\n",
    "            x = self.attn(x_cls, u, u, key_padding_mask=padding_mask)[0]  # (1, batch, embed_dim)\n",
    "        else:\n",
    "            residual = x\n",
    "            x = self.pre_attn_norm(x)\n",
    "            x = self.attn(x, x, x, key_padding_mask=padding_mask,\n",
    "                          attn_mask=attn_mask)[0]  # (seq_len, batch, embed_dim)\n",
    "            if self.return_pre_softmax:\n",
    "                pre_softmax_attention = self.attn(x, x, x, key_padding_mask=padding_mask, \n",
    "                    attn_mask=attn_mask, average_attn_weights=False)[2]\n",
    "                pre_softmax_interaction = self.attn(x, x, x, key_padding_mask=padding_mask,\n",
    "                    attn_mask=attn_mask, average_attn_weights=False)[3]\n",
    "                pre_softmax_attention.cpu().detach()\n",
    "                pre_softmax_interaction.cpu().detach()\n",
    "                self.pre_softmax_attention = pre_softmax_attention\n",
    "                self.pre_softmax_interaction = pre_softmax_interaction\n",
    "            \n",
    "        if padding_mask is not None:\n",
    "            self.padding_mask = padding_mask\n",
    "\n",
    "        if self.c_attn is not None:\n",
    "            tgt_len = x.size(0)\n",
    "            x = x.view(tgt_len, -1, self.num_heads, self.head_dim)\n",
    "            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)\n",
    "            x = x.reshape(tgt_len, -1, self.embed_dim)\n",
    "        if self.post_attn_norm is not None:\n",
    "            x = self.post_attn_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.pre_fc_norm(x)\n",
    "        # reshape to tokens-first for routing\n",
    "        seq_len, batch_size, embed_dim = x.shape\n",
    "        tokens = x.reshape(seq_len * batch_size, embed_dim)\n",
    "        with torch.no_grad():\n",
    "            router_logits = self.router(tokens)\n",
    "            if self.training and self.moe_router_jitter > 0:\n",
    "                noise = torch.empty_like(router_logits).uniform_(0, 1)\n",
    "                noise = -torch.log(-torch.log(noise.clamp(min=1e-9)))\n",
    "                router_logits = router_logits + self.moe_router_jitter * noise\n",
    "            gates = torch.softmax(router_logits, dim=-1)\n",
    "            # Load-balancing aux: generalized for top-k routing\n",
    "            mean_prob_per_expert = gates.mean(dim=0)\n",
    "            if self.moe_top_k == 1:\n",
    "                token_choice = gates.argmax(dim=-1)\n",
    "                frac_per_expert = torch.stack([\n",
    "                    (token_choice == e).float().mean() for e in range(self.moe_num_experts)\n",
    "                ])\n",
    "            else:\n",
    "                topk_idx_aux = gates.topk(k=min(self.moe_top_k, self.moe_num_experts), dim=-1).indices\n",
    "                assigned = torch.zeros_like(gates)\n",
    "                assigned.scatter_(1, topk_idx_aux, 1.0)\n",
    "                # Normalize so sum_e frac_per_expert ~= 1 like top-1 case\n",
    "                frac_per_expert = assigned.mean(dim=0) / float(self.moe_top_k)\n",
    "            aux = (mean_prob_per_expert * frac_per_expert).sum() * (self.moe_num_experts ** 2)\n",
    "            self._last_aux_loss = aux.detach()\n",
    "\n",
    "        output_tokens = torch.zeros_like(tokens)\n",
    "        if self.moe_top_k == 1:\n",
    "            # top-1 routing (Switch-style)\n",
    "            top1_idx = gates.argmax(dim=-1)\n",
    "            top1_w = gates.gather(1, top1_idx.unsqueeze(1)).squeeze(1)\n",
    "            capacity = int(self.moe_capacity_factor * math.ceil(tokens.size(0) / max(1, self.moe_num_experts)))\n",
    "            for e in range(self.moe_num_experts):\n",
    "                mask = (top1_idx == e)\n",
    "                if mask.any():\n",
    "                    idx = torch.nonzero(mask, as_tuple=False).squeeze(1)\n",
    "                    if idx.numel() > capacity:\n",
    "                        sel = top1_w[idx].topk(capacity, sorted=False).indices\n",
    "                        idx = idx[sel]\n",
    "                    expert_in = tokens.index_select(0, idx)\n",
    "                    expert_out = self.experts[e](expert_in)\n",
    "                    expert_w = top1_w.index_select(0, idx).unsqueeze(1)\n",
    "                    expert_out = expert_out * expert_w\n",
    "                    output_tokens.index_copy_(0, idx, expert_out)\n",
    "        else:\n",
    "            # top-k routing: send tokens to k experts and sum weighted outputs\n",
    "            k = min(self.moe_top_k, self.moe_num_experts)\n",
    "            topk_vals, topk_idx = gates.topk(k=k, dim=-1)\n",
    "            # save topk_idx for hooking, analysis\n",
    "            self.topk_idx = topk_idx\n",
    "            # Renormalize selected gates to sum to 1 per token\n",
    "            denom = topk_vals.sum(dim=1, keepdim=True).clamp(min=1e-9)\n",
    "            topk_w = topk_vals / denom\n",
    "            # save topk_w for hooking, analysis\n",
    "            self.topk_w = topk_w\n",
    "            # Capacity per expert accounts for k assignments per token\n",
    "            capacity = int(self.moe_capacity_factor * math.ceil((tokens.size(0) * k) / max(1, self.moe_num_experts)))\n",
    "            for e in range(self.moe_num_experts):\n",
    "                mask_e = (topk_idx == e)\n",
    "                if mask_e.any():\n",
    "                    rows, cols = torch.nonzero(mask_e, as_tuple=True)\n",
    "                    if rows.numel() > capacity:\n",
    "                        weights_e = topk_w[rows, cols]\n",
    "                        sel = weights_e.topk(capacity, sorted=False).indices\n",
    "                        rows = rows.index_select(0, sel)\n",
    "                        cols = cols.index_select(0, sel)\n",
    "                        weights_e = weights_e.index_select(0, sel)\n",
    "                    else:\n",
    "                        weights_e = topk_w[rows, cols]\n",
    "                    expert_in = tokens.index_select(0, rows)\n",
    "                    expert_out = self.experts[e](expert_in)\n",
    "                    expert_out = expert_out * weights_e.unsqueeze(1)\n",
    "                    # Multiple experts can contribute to the same token; accumulate\n",
    "                    output_tokens.index_add_(0, rows, expert_out)\n",
    "        x = output_tokens.view(seq_len, batch_size, embed_dim)\n",
    "        x = self.dropout(x)\n",
    "        if self.w_resid is not None:\n",
    "            residual = torch.mul(self.w_resid, residual)\n",
    "        x += residual\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MoeParticleTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 #interpretability of pre-softmax\n",
    "                 return_pre_softmax=False,\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 moe_num_experts=4,\n",
    "                 moe_top_k=1,\n",
    "                 moe_layers=None,\n",
    "                 moe_capacity_factor=1.25,\n",
    "                 moe_aux_loss_coef=0.01,\n",
    "                 moe_router_jitter=0.0,\n",
    "                 **kwargs\n",
    "                 ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.for_inference = for_inference\n",
    "        self.use_amp = use_amp\n",
    "        self.return_pre_softmax = return_pre_softmax\n",
    "\n",
    "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
    "        self.default_cfg = default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
    "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                           add_bias_kv=False, activation=activation,\n",
    "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True)\n",
    "\n",
    "        self.cfg_block = cfg_block = copy.deepcopy(default_cfg)\n",
    "        if block_params is not None:\n",
    "            cfg_block.update(block_params)\n",
    "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
    "\n",
    "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
    "        if cls_block_params is not None:\n",
    "            cfg_cls_block.update(cls_block_params)\n",
    "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
    "\n",
    "        self.pair_extra_dim = pair_extra_dim\n",
    "        self.moe_num_experts = moe_num_experts\n",
    "        self.moe_top_k = moe_top_k\n",
    "        self.moe_layers = set(moe_layers) if moe_layers is not None else set()\n",
    "        self.moe_capacity_factor = moe_capacity_factor\n",
    "        self.moe_aux_loss_coef = moe_aux_loss_coef\n",
    "        self.moe_router_jitter = moe_router_jitter\n",
    "        if not (self.moe_num_experts and self.moe_num_experts > 0):\n",
    "            raise ValueError('moe_num_experts must be >= 1 for MoE-only MoeParticleTransformer')\n",
    "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
    "        self.pair_embed = PairEmbed(\n",
    "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
    "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
    "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
    "        blocks = []\n",
    "        for _ in range(num_layers):\n",
    "            blocks.append(Block(\n",
    "                **cfg_block,\n",
    "                moe_num_experts=self.moe_num_experts,\n",
    "                moe_top_k=self.moe_top_k,\n",
    "                moe_capacity_factor=self.moe_capacity_factor,\n",
    "                moe_aux_loss_coef=self.moe_aux_loss_coef,\n",
    "                moe_router_jitter=self.moe_router_jitter,\n",
    "                return_pre_softmax=self.return_pre_softmax\n",
    "            ))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.cls_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                **cfg_cls_block,\n",
    "                moe_num_experts=self.moe_num_experts,\n",
    "                moe_top_k=self.moe_top_k,\n",
    "                moe_capacity_factor=self.moe_capacity_factor,\n",
    "                moe_aux_loss_coef=self.moe_aux_loss_coef,\n",
    "                moe_router_jitter=self.moe_router_jitter,\n",
    "            ) for _ in range(num_cls_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        if fc_params is not None:\n",
    "            fcs = []\n",
    "            in_dim = embed_dim\n",
    "            for out_dim, drop_rate in fc_params:\n",
    "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
    "                in_dim = out_dim\n",
    "            fcs.append(nn.Linear(in_dim, num_classes))\n",
    "            self.fc = nn.Sequential(*fcs)\n",
    "        else:\n",
    "            self.fc = None\n",
    "\n",
    "        # init\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token', }\n",
    "\n",
    "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
    "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not self.for_inference:\n",
    "                if uu_idx is not None:\n",
    "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
    "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
    "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            # input embedding\n",
    "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
    "            attn_mask = None\n",
    "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
    "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
    "\n",
    "            # transform\n",
    "            self._moe_aux_loss = torch.tensor(0.0, device=x.device)\n",
    "            for block in self.blocks:\n",
    "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
    "                aux = getattr(block, '_last_aux_loss', None)\n",
    "                if aux is not None:\n",
    "                    self._moe_aux_loss = self._moe_aux_loss + self.moe_aux_loss_coef * aux\n",
    "\n",
    "            # extract class token\n",
    "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
    "            for block in self.cls_blocks:\n",
    "                cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
    "\n",
    "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
    "\n",
    "            # fc\n",
    "            if self.fc is None:\n",
    "                return x_cls\n",
    "            output = self.fc(x_cls)\n",
    "            if self.for_inference:\n",
    "                output = torch.softmax(output, dim=1)\n",
    "            # print('output:\\n', output)\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1754e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MoeParticleTransformerTagger(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pf_input_dim,\n",
    "                 sv_input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # interpretability of pre-softmax\n",
    "                 return_pre_softmax=False,\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.use_amp = use_amp\n",
    "\n",
    "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "\n",
    "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
    "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
    "\n",
    "        self.part = MoeParticleTransformer(input_dim=embed_dims[-1],\n",
    "                                        num_classes=num_classes,\n",
    "                                        # network configurations\n",
    "                                        pair_input_dim=pair_input_dim,\n",
    "                                        pair_extra_dim=pair_extra_dim,\n",
    "                                        remove_self_pair=remove_self_pair,\n",
    "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
    "                                        embed_dims=[],\n",
    "                                        pair_embed_dims=pair_embed_dims,\n",
    "                                        num_heads=num_heads,\n",
    "                                        num_layers=num_layers,\n",
    "                                        num_cls_layers=num_cls_layers,\n",
    "                                        block_params=block_params,\n",
    "                                        cls_block_params=cls_block_params,\n",
    "                                        fc_params=fc_params,\n",
    "                                        activation=activation,\n",
    "                                        # misc\n",
    "                                        trim=False,\n",
    "                                        for_inference=for_inference,\n",
    "                                        use_amp=use_amp)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'part.cls_token', }\n",
    "\n",
    "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pf_x, pf_v, pf_mask, _ = self.pf_trimmer(pf_x, pf_v, pf_mask)\n",
    "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
    "            v = torch.cat([pf_v, sv_v], dim=2)\n",
    "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
    "            sv_x = self.sv_embed(sv_x)\n",
    "            x = torch.cat([pf_x, sv_x], dim=0)\n",
    "\n",
    "            return self.part(x, v, mask)\n",
    "\n",
    "\n",
    "class MoeParticleTransformerTaggerWithExtraPairFeatures(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pf_input_dim,\n",
    "                 sv_input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.use_amp = use_amp\n",
    "        self.for_inference = for_inference\n",
    "\n",
    "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "\n",
    "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
    "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
    "\n",
    "        self.part = MoeParticleTransformer(input_dim=embed_dims[-1],\n",
    "                                        num_classes=num_classes,\n",
    "                                        # network configurations\n",
    "                                        pair_input_dim=pair_input_dim,\n",
    "                                        pair_extra_dim=pair_extra_dim,\n",
    "                                        remove_self_pair=remove_self_pair,\n",
    "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
    "                                        embed_dims=[],\n",
    "                                        pair_embed_dims=pair_embed_dims,\n",
    "                                        num_heads=num_heads,\n",
    "                                        num_layers=num_layers,\n",
    "                                        num_cls_layers=num_cls_layers,\n",
    "                                        block_params=block_params,\n",
    "                                        cls_block_params=cls_block_params,\n",
    "                                        fc_params=fc_params,\n",
    "                                        activation=activation,\n",
    "                                        # misc\n",
    "                                        trim=False,\n",
    "                                        for_inference=for_inference,\n",
    "                                        use_amp=use_amp)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'part.cls_token', }\n",
    "\n",
    "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None, pf_uu=None, pf_uu_idx=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not self.for_inference:\n",
    "                if pf_uu_idx is not None:\n",
    "                    pf_uu = build_sparse_tensor(pf_uu, pf_uu_idx, pf_x.size(-1))\n",
    "\n",
    "            pf_x, pf_v, pf_mask, pf_uu = self.pf_trimmer(pf_x, pf_v, pf_mask, pf_uu)\n",
    "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
    "            v = torch.cat([pf_v, sv_v], dim=2)\n",
    "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
    "            uu = torch.zeros(v.size(0), pf_uu.size(1), v.size(2), v.size(2), dtype=v.dtype, device=v.device)\n",
    "            uu[:, :, :pf_x.size(2), :pf_x.size(2)] = pf_uu\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
    "            sv_x = self.sv_embed(sv_x)\n",
    "            x = torch.cat([pf_x, sv_x], dim=0)\n",
    "\n",
    "            return self.part(x, v, mask, uu)\n",
    "\n",
    "class MoeParticleTransformerWrapper(torch.nn.Module):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.mod = MoeParticleTransformer(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'mod.cls_token', }\n",
    "\n",
    "    def forward(self, points, features, lorentz_vectors, mask):\n",
    "        return self.mod(features, v=lorentz_vectors, mask=mask)\n",
    "\n",
    "\n",
    "def get_model(data_type=None, **kwargs):\n",
    "\n",
    "    if data_type == 'jc_full' or None:\n",
    "        cfg = dict(\n",
    "            input_dim=17,\n",
    "            num_classes=10,\n",
    "            # network configurations\n",
    "            pair_input_dim=4,\n",
    "            use_pre_activation_pair=False,\n",
    "            embed_dims=[128, 512, 128],\n",
    "            pair_embed_dims=[64, 64, 64],\n",
    "            num_heads=8,\n",
    "            num_layers=8,\n",
    "            num_cls_layers=2,\n",
    "            block_params=None,\n",
    "            cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "            fc_params=[],\n",
    "            activation='gelu',\n",
    "            # misc\n",
    "            trim=True,\n",
    "            for_inference=False,\n",
    "            # interpretability of pre-softmax\n",
    "            return_pre_softmax=True,\n",
    "            # MoE settings\n",
    "            moe_num_experts=8,\n",
    "            moe_top_k=2,\n",
    "            moe_capacity_factor=1.25,\n",
    "            moe_aux_loss_coef=0.01,\n",
    "            moe_router_jitter=0.0,\n",
    "        )\n",
    "    cfg.update(**kwargs)\n",
    "\n",
    "    model = MoeParticleTransformerWrapper(**cfg)\n",
    "\n",
    "    model_info = {\n",
    "        }\n",
    "    return model, model_info\n",
    "\n",
    "\n",
    "def get_loss(data_config, **kwargs):\n",
    "    return torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "399d3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pre_Softmax_Hook:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.kwargs = self.model.kwargs\n",
    "\n",
    "        if isinstance(self.model, MoeParticleTransformerWrapper):\n",
    "            self.model = self.model.mod\n",
    "\n",
    "        for module in self.model.blocks:\n",
    "            #if layer_name in name:\n",
    "                # register forward hook functions\n",
    "            handle_attn = module.register_forward_hook(lambda *args, **kwargs: Pre_Softmax_Hook.get_pre_softmax_attention(self, *args, **kwargs))\n",
    "            handle_inter = module.register_forward_hook(lambda *args, **kwargs: Pre_Softmax_Hook.get_pre_softmax_interaction(self, *args, **kwargs))\n",
    "\n",
    "            print(f\"Registered hook onto module\")\n",
    "\n",
    "        self.handle_attn = handle_attn\n",
    "        self.handle_inter = handle_inter\n",
    "\n",
    "        self.embed_dim = self.model.default_cfg['embed_dim']\n",
    "        self.num_heads = self.model.default_cfg['num_heads']\n",
    "        self.seq_len = self.embed_dim\n",
    "\n",
    "        self.pre_softmax_attentions = torch.empty((0, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)\n",
    "        self.pre_softmax_interactions = torch.empty((0, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)\n",
    "\n",
    "    # hooks will grab from the outputs of Block\n",
    "    # meaning we don't need to modify forward methods for anything else\n",
    "    # 3 hours later -- lol this was wrong\n",
    "\n",
    "    def get_pre_softmax_attention(self, module, input, output):\n",
    "        print('Getting pre_softmax attention...')\n",
    "\n",
    "        # handle batching - we will divide 1st dimension by the number such that it will comport with num_heads\n",
    "        #print(f'Got shape:{output[1].shape}')\n",
    "        output_hooked = module.pre_softmax_attention\n",
    "        output_split = output_hooked.view(output_hooked.shape[0]//self.num_heads, self.num_heads, output_hooked.shape[1], output_hooked.shape[2])\n",
    "        output_unsqueezed = output_split.unsqueeze(dim=0)\n",
    "    \n",
    "        #print('Split the output into heads.\\nNew Tensor Shapes:')\n",
    "        #print(f'{output_split[0].shape}')\n",
    "\n",
    "        # set correct number of particles for cat\n",
    "\n",
    "        if self.pre_softmax_attentions.shape[0] == 0:\n",
    "            self.pre_softmax_attentions = torch.empty((0, output_hooked.shape[0]//self.num_heads, self.num_heads, output_unsqueezed.shape[3], output_unsqueezed.shape[4]), dtype=torch.float32)\n",
    "\n",
    "        self.pre_softmax_attentions = torch.cat((self.pre_softmax_attentions, output_unsqueezed), dim=0)\n",
    "        \n",
    "        #self.sorted_attentions = self.sort(self.pre_softmax_attentions)\n",
    "\n",
    "    def get_pre_softmax_interaction(self, module, input, output):\n",
    "        #print('Getting pre-softmax interaction...')\n",
    "\n",
    "        # handle batching - we will divide 1st dimension by the number such that it will comport with num_heads\n",
    "        output_hooked = module.pre_softmax_attention\n",
    "        output_split = output_hooked.view(output_hooked.shape[0]//self.num_heads, self.num_heads, output_hooked.shape[1], output_hooked.shape[2])\n",
    "        output_unsqueezed = output_split.unsqueeze(dim=0)\n",
    "\n",
    "        # set correct number of particles for cat\n",
    "        if self.pre_softmax_interactions.shape[0] == 0:\n",
    "            self.pre_softmax_interactions = torch.empty((0, output_hooked.shape[0]//self.num_heads, self.num_heads, output_unsqueezed.shape[3], output_unsqueezed.shape[4]), dtype=torch.float32)\n",
    "\n",
    "        self.pre_softmax_interactions = torch.cat((self.pre_softmax_interactions, output_unsqueezed), dim=0)\n",
    "        \n",
    "    def sort(self, tensor):\n",
    "        \n",
    "        config = self.kwargs\n",
    "\n",
    "        if 'num_layers' in config:\n",
    "            num_layers = config['num_layers']\n",
    "        else:\n",
    "            num_layers = 8\n",
    "\n",
    "        num_jets = tensor.shape[0] // num_layers\n",
    "\n",
    "        tensor_as_np = tensor.numpy() # should be (total_num_layers, num_heads, jet_length, jet_length)\n",
    "        tensor_as_np = np.split(tensor_as_np, indices_or_sections=num_jets, axis=0) # now listed by layer -> (jet_num, head, jet_length, jet_length)\n",
    "\n",
    "        return tensor_as_np\n",
    "\n",
    "    def cut_padding(self, tensor, mask):\n",
    "        r'''\n",
    "        Presents collected tensor from Pre_Softmax_Hook as list of particles with each item a 4d ndarray like (layers, heads, jet_length, jet_length).\n",
    "        Padding removed.\n",
    "\n",
    "        Args:\n",
    "        - tensor: The input tensor to process.\n",
    "        - config: Model configuration dict. If 'num_layers' is not present, defaults to 8 as in original ParT.\n",
    "\n",
    "        Outputs:\n",
    "        tensor_as_np: list of jagged arrays. Each item in the list is a jet's full with shape (layers, heads, jet_length, jet_length)\n",
    "        '''\n",
    "        config = self.kwargs\n",
    "\n",
    "        if 'num_layers' in config:\n",
    "            num_layers = config['num_layers']\n",
    "        else:\n",
    "            num_layers = 8\n",
    "\n",
    "        num_jets = tensor.shape[0] // num_layers\n",
    "        \n",
    "        tensor_as_np = tensor.numpy() # should be (layers, jets, heads, jet_length, jet_length)\n",
    "        \n",
    "        tensor_as_ak = ak.from_numpy(tensor_as_np)\n",
    "\n",
    "        padding_limits = []\n",
    "\n",
    "        for jet_idx in range(tensor_as_np.shape[1]):\n",
    "            padding_limit = np.sum(mask[jet_idx]).astype(int)\n",
    "            padding_limits.append(padding_limit)\n",
    "        \n",
    "        print(f'Padding Limit: {padding_limits}')\n",
    "\n",
    "        return padding_limits\n",
    "\n",
    "    def clear(self):\n",
    "        self.pre_softmax_attentions = torch.empty((0, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)\n",
    "        self.pre_softmax_interactions = torch.empty((0, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)\n",
    "        self.handle_attn.remove()\n",
    "        self.handle_inter.remove()\n",
    "        #self.cls_handle_attn.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "08e5dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router_Hook:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.kwargs = self.model.kwargs\n",
    "\n",
    "        if isinstance(self.model, MoeParticleTransformerWrapper):\n",
    "            self.model = self.model.mod\n",
    "\n",
    "        #for module in self.model.blocks[:0]:  # only hook the last MoE layer\n",
    "        collect_assignments = self.model.blocks[0].register_forward_hook(lambda *args, **kwargs: Router_Hook.collect_expert_assignments(self, *args, **kwargs))\n",
    "        print(f\"Registered hook onto particle module\")    \n",
    "        for module in self.model.cls_blocks[-1:]:  # only hook the last MoE layer\n",
    "            cls_collect_assignments = module.register_forward_hook(lambda *args, **kwargs: Router_Hook.cls_collect_expert_assignments(self, *args, **kwargs))\n",
    "            print(f\"Registered hook onto class module\")\n",
    "\n",
    "        self.expert_weights = torch.empty((0, self.model.moe_num_experts), dtype=torch.float32)\n",
    "        self.expert_assignments = torch.empty((0, self.model.moe_num_experts), dtype=torch.float32)\n",
    "\n",
    "        self.cls_expert_weights = torch.empty((0, self.model.moe_num_experts), dtype=torch.float32)\n",
    "        self.cls_expert_assignments = torch.empty((0, self.model.moe_num_experts), dtype=torch.float32)\n",
    "\n",
    "    def collect_expert_assignments(self, module, input, output):\n",
    "        print('Collecting expert assignments')\n",
    "\n",
    "        expert_weights = module.topk_w # shape (num_particles, k_experts)\n",
    "        expert_assignments = module.topk_idx # shape (num_particles, k_experts)\n",
    "\n",
    "        # handle padding by not recording data from null particles\n",
    "        if hasattr(module, 'padding_mask'):\n",
    "            if module.padding_mask is not None:\n",
    "                print('Found padding mask, applying to expert assignments')\n",
    "                padding_mask = module.padding_mask # shape (batch_size, seq_len)\n",
    "                padding_mask = padding_mask.view(-1) # shape (batch_size * seq_len)\n",
    "                valid_indices = torch.where(padding_mask == 0)[0] # indices of non-padded particles\n",
    "                self.valid_indices = valid_indices\n",
    "                expert_weights = expert_weights[valid_indices,:]\n",
    "                expert_assignments = expert_assignments[valid_indices,:]\n",
    "                print(f'After padding, expert weights shape: {expert_weights.shape}')\n",
    "\n",
    "        # sort weights according to assignments\n",
    "        for part_idx in range(expert_assignments.shape[0]):\n",
    "            assigned = expert_assignments[part_idx,:]\n",
    "            weights = expert_weights[part_idx,:]\n",
    "\n",
    "            sparse_assignments = np.array([i in assigned for i in range(self.model.moe_num_experts)])\n",
    "            sparse_weights = np.zeros(sparse_assignments.shape)\n",
    "            for idx, expert_idx in enumerate(assigned): # need to keep the weights assigned to each expert consistent\n",
    "                sparse_weights[expert_idx]=weights[idx]\n",
    "\n",
    "            self.expert_assignments = torch.cat((self.expert_assignments, torch.from_numpy(sparse_assignments).unsqueeze(0)), dim=0)\n",
    "            self.expert_weights = torch.cat((self.expert_weights, torch.from_numpy(sparse_weights).unsqueeze(0)), dim=0)\n",
    "\n",
    "    def cls_collect_expert_assignments(self, module, input, output):\n",
    "        print('Collecting class expert assignments')\n",
    "\n",
    "        expert_weights = module.topk_w # shape (num_particles, k_experts)\n",
    "        expert_assignments = module.topk_idx # shape (num_particles, k_experts)\n",
    "        # sort weights according to assignments\n",
    "        for part_idx in range(expert_assignments.shape[0]):\n",
    "            assigned = expert_assignments[part_idx,:]\n",
    "            weights = expert_weights[part_idx,:]\n",
    "\n",
    "            sparse_assignments = np.array([i in assigned for i in range(self.model.moe_num_experts)])\n",
    "            sparse_weights = np.zeros(sparse_assignments.shape)\n",
    "            for idx, expert_idx in enumerate(assigned): # need to keep the weights assigned to each expert consistent\n",
    "                sparse_weights[expert_idx]=weights[idx]\n",
    "\n",
    "            self.cls_expert_assignments = torch.cat((self.cls_expert_assignments, torch.from_numpy(sparse_assignments).unsqueeze(0)), dim=0)\n",
    "            self.cls_expert_weights = torch.cat((self.cls_expert_weights, torch.from_numpy(sparse_weights).unsqueeze(0)), dim=0)\n",
    "\n",
    "    def clear(self):\n",
    "        self.collect_assignments.remove()\n",
    "        self.cls_collect_expert_assignments.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93fa23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "726a21aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered hook onto particle module\n",
      "Registered hook onto class module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n"
     ]
    }
   ],
   "source": [
    "MoE_model = get_model('jc_full')[0]\n",
    "\n",
    "MoE_statedict = torch.load('net_best_epoch_state.pt', map_location=torch.device('cpu'))\n",
    "MoE_model.load_state_dict(MoE_statedict)\n",
    "router_hook = Router_Hook(model=MoE_model)\n",
    "pre_softmax_hook = Pre_Softmax_Hook(model=MoE_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ebf44848",
   "metadata": {},
   "outputs": [],
   "source": [
    "howmanyjets = 100\n",
    "\n",
    "features = np.load('jc_full_data/jc_full_pf_features.npy', allow_pickle=True)[:howmanyjets]\n",
    "masks = np.load('jc_full_data/jc_full_pf_mask.npy', allow_pickle=True)[:howmanyjets]\n",
    "labels = np.load('jc_full_data/jc_full_labels.npy', allow_pickle=True)[:howmanyjets]\n",
    "vectors = np.load('jc_full_data/jc_full_pf_vectors.npy', allow_pickle=True)[:howmanyjets]\n",
    "points = np.load('jc_full_data/jc_full_pf_points.npy', allow_pickle=True)[:howmanyjets]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In pod: pull multiple jet types\n",
    "jet_type = 'Z QQ'\n",
    "start_idx = 90000\n",
    "howmanyjets = 1000\n",
    "\n",
    "features = np.load('/moe-interpretability-pv/datasets/jc_full_pf_features.npy', allow_pickle=True)[start_idx:howmanyjets+start_idx]\n",
    "masks = np.load('/moe-interpretability-pv/datasets/jc_full_pf_mask.npy', allow_pickle=True)[start_idx:howmanyjets+start_idx]\n",
    "labels = np.load('/moe-interpretability-pv/datasets/jc_full_labels.npy', allow_pickle=True)[start_idx:howmanyjets+start_idx]\n",
    "vectors = np.load('/moe-interpretability-pv/datasets/jc_full_pf_vectors.npy', allow_pickle=True)[start_idx:howmanyjets+start_idx]\n",
    "points = np.load('/moe-interpretability-pv/datasets/jc_full_pf_points.npy', allow_pickle=True)[start_idx:howmanyjets+start_idx]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2181a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 17, 128) (100, 1, 128) (100, 10) (100, 4, 128)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape, masks.shape, labels.shape, vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29e07e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0\n",
      "Lowest sum of masks is 24 at 9\n"
     ]
    }
   ],
   "source": [
    "print(sum(masks[9,0,:]))\n",
    "\n",
    "lowest_sum = 999\n",
    "\n",
    "for jet_idx, jet_mask in enumerate(masks):\n",
    "    if sum(masks[jet_idx,0,:]) < lowest_sum:\n",
    "        lowest_sum, lowest_idx = sum(masks[jet_idx,0,:]), jet_idx\n",
    "\n",
    "lowest_sum = int(lowest_sum)\n",
    "print(f'Lowest sum of masks is {lowest_sum} at {lowest_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e356232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 100, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n"
     ]
    }
   ],
   "source": [
    "# check distribution of labels\n",
    "label_dist = {i: [] for i in range(labels.shape[1])}\n",
    "for label in range(labels.shape[1]):\n",
    "    label_dist[label] = np.sum(labels[:,label])\n",
    "print(label_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just testing jet 9, since it has lowest num_parts\n",
    "test_jet = 9\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred= MoE_model(torch.from_numpy(points[test_jet]).unsqueeze(0),torch.from_numpy(features[test_jet]).unsqueeze(0),\n",
    "                                torch.from_numpy(vectors[test_jet]).unsqueeze(0),torch.from_numpy(masks[test_jet]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a59cf004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1962/514037021.py:689: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
      "/tmp/ipykernel_1962/2705280004.py:20: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting expert assignments\n",
      "Found padding mask, applying to expert assignments\n",
      "After padding, expert weights shape: torch.Size([4130, 2])\n",
      "Getting pre_softmax attention...\n",
      "Getting pre_softmax attention...\n",
      "Getting pre_softmax attention...\n",
      "Getting pre_softmax attention...\n",
      "Getting pre_softmax attention...\n",
      "Getting pre_softmax attention...\n",
      "Getting pre_softmax attention...\n",
      "Getting pre_softmax attention...\n",
      "Collecting class expert assignments\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred= MoE_model(torch.from_numpy(points),torch.from_numpy(features),\n",
    "                                torch.from_numpy(vectors),torch.from_numpy(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "49275463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 1.]])\n",
      "torch.Size([4130, 8])\n"
     ]
    }
   ],
   "source": [
    "print(router_hook.expert_assignments[:20])\n",
    "print(router_hook.expert_assignments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d309c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "particle_assignments = np.array(router_hook.expert_assignments)\n",
    "particle_assignments_hist = np.sum(particle_assignments, axis=0)\n",
    "np.save('/moe-interpretability-pv/particle_expert_assignments_label1.npy', particle_assignments)\n",
    "plt.bar(np.arange(router_hook.model.moe_num_experts), particle_assignments_hist)\n",
    "plt.xlabel('Expert Index')\n",
    "plt.ylabel('Number of Particles Assigned')\n",
    "plt.title('Particle Expert Assignments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6232690a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.5502, 0.0000, 0.0000, 0.0000, 0.4498],\n",
      "        [0.5054, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4946],\n",
      "        [0.0000, 0.0000, 0.5057, 0.0000, 0.0000, 0.4943, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5475, 0.4525, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4186, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5814],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4812, 0.0000, 0.0000, 0.0000, 0.5188],\n",
      "        [0.0000, 0.0000, 0.5401, 0.4599, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4584, 0.0000, 0.0000, 0.0000, 0.5416],\n",
      "        [0.0000, 0.0000, 0.4523, 0.0000, 0.0000, 0.0000, 0.0000, 0.5477],\n",
      "        [0.4873, 0.0000, 0.0000, 0.0000, 0.0000, 0.5127, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4858, 0.5142, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4809, 0.5191, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5261, 0.0000, 0.4739, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4657, 0.0000, 0.0000, 0.0000, 0.5343],\n",
      "        [0.0000, 0.0000, 0.4737, 0.0000, 0.5263, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5277, 0.0000, 0.0000, 0.0000, 0.0000, 0.4723],\n",
      "        [0.0000, 0.0000, 0.5245, 0.0000, 0.4755, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4793, 0.5207, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5741, 0.4259, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4915, 0.5085, 0.0000, 0.0000, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([100, 8])\n"
     ]
    }
   ],
   "source": [
    "print(router_hook.cls_expert_weights[:20])\n",
    "print(router_hook.cls_expert_assignments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2382da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_assignments = np.array(router_hook.cls_expert_assignments)\n",
    "class_assignments_hist = np.sum(class_assignments, axis=0)\n",
    "np.save('/moe-interpretability-pv/particle_class_assignments_label1.npy', particle_assignments)\n",
    "plt.bar(np.arange(router_hook.model.moe_num_experts), class_assignments_hist)\n",
    "plt.xlabel('Expert Index')\n",
    "plt.ylabel('Number of Particles Assigned')\n",
    "plt.title('Particle Expert Assignments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAN: Sort by assigned expert\n",
    "# Define feature subspaces to search for correlations e.g. angular space, momentum, charge, particle type\n",
    "# Compute average norm between all particles of given expert in these subspaces\n",
    "# In the average, weight by the expert's weighting\n",
    "# Could do scatterplots to demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83590e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angular subspace: (eta, phi) at indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "45be2134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12800, 17)\n",
      "[ 1.9998076   1.8451487   1.7912426   1.8055545  -0.32082492  0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.07607916  0.09253383]\n",
      "[ 1.9998076   1.8451487   1.7912426   1.8055545  -0.32082492  0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.07607916  0.09253383]\n",
      "True should be true if no padded particles\n",
      "True\n",
      "(4130, 17)\n"
     ]
    }
   ],
   "source": [
    "# Charge, particle types at indices 5 and 6-10 (ChargedHadron, NeutralHadron, Photon, Electron, Muon)\n",
    "\n",
    "flat_features = features.transpose(0,2,1) # (N, C, P) -> (N, P, C)\n",
    "\n",
    "# features: (N, P, C) -> (N*P, C)\n",
    "flat_features = flat_features.reshape(-1, features.shape[1])\n",
    "\n",
    "print(flat_features.shape)\n",
    "print(features[0,:,1])\n",
    "print(flat_features[1,:])\n",
    "flat_features = flat_features[router_hook.valid_indices,:]\n",
    "print(f'{not ~flat_features[:,0].any()} should be true if no padded particles')\n",
    "print(flat_features[-100:,0].any())\n",
    "print(flat_features.shape)\n",
    "\n",
    "assignments = {i: [] for i in range(router_hook.model.moe_num_experts)}\n",
    "weights = {i: [] for i in range(router_hook.model.moe_num_experts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0bcce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in range(len(flat_features)):\n",
    "    for expert in range(router_hook.model.moe_num_experts):\n",
    "        if router_hook.expert_assignments[part, expert] == 1:\n",
    "            assignments[expert].append(flat_features[part,:])\n",
    "            weights[expert].append(router_hook.expert_weights[part, expert])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1971dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.9998076   1.8451487   1.7912426   1.8055545  -0.32082492  0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.07607916  0.09253383]\n"
     ]
    }
   ],
   "source": [
    "print(assignments[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "804efe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then look at assignments, which combinations are the most common?\n",
    "# Probably create 2D grid to demonstrate\n",
    "\n",
    "expert_pairs = np.zeros((router_hook.model.moe_num_experts, router_hook.model.moe_num_experts))\n",
    "\n",
    "for part in router_hook.expert_assignments:\n",
    "    assigned_experts = np.where(part==1)[0]\n",
    "    expert_pairs[assigned_experts[0], assigned_experts[1]] += 1\n",
    "    expert_pairs[assigned_experts[1], assigned_experts[0]] += 1\n",
    "    # normalize to fractions of total particles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85620101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 9.200e+01 1.900e+01 5.400e+01 3.870e+02 2.667e+03 0.000e+00\n",
      "  1.180e+02]\n",
      " [9.200e+01 0.000e+00 9.600e+01 1.400e+01 3.200e+01 2.552e+03 0.000e+00\n",
      "  2.430e+02]\n",
      " [1.900e+01 9.600e+01 0.000e+00 6.500e+01 1.860e+02 2.569e+03 3.000e+00\n",
      "  2.050e+02]\n",
      " [5.400e+01 1.400e+01 6.500e+01 0.000e+00 4.110e+02 7.400e+02 0.000e+00\n",
      "  4.840e+02]\n",
      " [3.870e+02 3.200e+01 1.860e+02 4.110e+02 0.000e+00 1.250e+03 6.000e+00\n",
      "  9.900e+02]\n",
      " [2.667e+03 2.552e+03 2.569e+03 7.400e+02 1.250e+03 0.000e+00 6.100e+01\n",
      "  5.339e+03]\n",
      " [0.000e+00 0.000e+00 3.000e+00 0.000e+00 6.000e+00 6.100e+01 0.000e+00\n",
      "  1.992e+03]\n",
      " [1.180e+02 2.430e+02 2.050e+02 4.840e+02 9.900e+02 5.339e+03 1.992e+03\n",
      "  0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(expert_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af1ede13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(expert_pairs, cmap='magma')\n",
    "\n",
    "rows, cols = expert_pairs.shape\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        if j != i:\n",
    "            text = ax.text(j, i, int(expert_pairs[i, j]),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\") # Customize text color if needed\n",
    "        if [i,j] == [5,7] or [i,j] == [7,5]:\n",
    "            text = ax.text(j, i, int(expert_pairs[i, j]),\n",
    "                       ha=\"center\", va=\"center\", color=\"black\") # Customize text color if needed\n",
    "\n",
    "plt.xlabel('Expert Index 1')\n",
    "plt.ylabel('Expert Index 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider high-level overviews: for each decay, plot expert distributions\n",
    "# consider for particle types, what experts like them\n",
    "# consider ranges of pT\n",
    "# consider angle off the jet-axis to indicate particular decay, also (px, py, pz) if we can get some good scaling\n",
    "# we'd like the model to recognize cross-section etc.\n",
    "# probably some automation to generate plots more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1120b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider looking at 'distinguisher' particles - e.g. large d(deltaR), high pt_rel, high e_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f2b3c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_by_expert = {i: [] for i in range(router_hook.model.moe_num_experts)}\n",
    "\n",
    "for expert in range(router_hook.model.moe_num_experts):\n",
    "    for part_idx, part_assignment in enumerate(router_hook.expert_assignments):\n",
    "        if router_hook.expert_assignments[part_idx, expert] == 1:\n",
    "            parts_by_expert[expert].append(flat_features[part_idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1b6e9948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.9998076   1.8451487   1.7912426   1.8055545  -0.32082492  0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.07607916  0.09253383]\n"
     ]
    }
   ],
   "source": [
    "print(parts_by_expert[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "542c4ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.9998076   1.8451487   1.7912426   1.8055545  -0.32082492  0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.07607916  0.09253383]\n"
     ]
    }
   ],
   "source": [
    "print(parts_by_expert[5][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "93a59565",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ids = {\n",
    "    0: 'part_pt_log',\n",
    "    1: 'part_e_log',\n",
    "    2: 'part_logptrel',\n",
    "    3: 'part_logerel',\n",
    "    4: 'part_deltaR',\n",
    "    5: 'part_charge',\n",
    "    6: 'part_isChargedHadron',\n",
    "    7: 'part_isNeutralHadron',\n",
    "    8: 'part_isPhoton',\n",
    "    9: 'part_isElectron',\n",
    "    10: 'part_isMuon',\n",
    "    11: 'part_d0',\n",
    "    12: 'part_d0err',\n",
    "    13: 'part_dz',\n",
    "    14: 'part_dzerr',\n",
    "    15: 'part_deta',\n",
    "    16: 'part_dphi'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a8ba9816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 0 particle types: {'part_isChargedHadron': 0.4538904899135447, 'part_isNeutralHadron': 0.0792507204610951, 'part_isPhoton': 0.4409221902017291, 'part_isElectron': 0.01729106628242075, 'part_isMuon': 0.008645533141210375}\n",
      "Expert 1 particle types: {'part_isChargedHadron': 0.43859649122807015, 'part_isNeutralHadron': 0.11802232854864433, 'part_isPhoton': 0.405103668261563, 'part_isElectron': 0.02073365231259968, 'part_isMuon': 0.017543859649122806}\n",
      "Expert 2 particle types: {'part_isChargedHadron': 0.4858156028368794, 'part_isNeutralHadron': 0.08687943262411348, 'part_isPhoton': 0.39361702127659576, 'part_isElectron': 0.02127659574468085, 'part_isMuon': 0.012411347517730497}\n",
      "Expert 3 particle types: {'part_isChargedHadron': 0.45791245791245794, 'part_isNeutralHadron': 0.11447811447811448, 'part_isPhoton': 0.4175084175084175, 'part_isElectron': 0.003367003367003367, 'part_isMuon': 0.006734006734006734}\n",
      "Expert 4 particle types: {'part_isChargedHadron': 0.4937888198757764, 'part_isNeutralHadron': 0.09161490683229814, 'part_isPhoton': 0.39751552795031053, 'part_isElectron': 0.009316770186335404, 'part_isMuon': 0.007763975155279503}\n",
      "Expert 5 particle types: {'part_isChargedHadron': 0.4733072916666667, 'part_isNeutralHadron': 0.0888671875, 'part_isPhoton': 0.4108072916666667, 'part_isElectron': 0.015299479166666666, 'part_isMuon': 0.01171875}\n",
      "Expert 6 particle types: {'part_isChargedHadron': 0.42857142857142855, 'part_isNeutralHadron': 0.11943793911007025, 'part_isPhoton': 0.44028103044496486, 'part_isElectron': 0.00468384074941452, 'part_isMuon': 0.00702576112412178}\n",
      "Expert 7 particle types: {'part_isChargedHadron': 0.46770025839793283, 'part_isNeutralHadron': 0.08940568475452196, 'part_isPhoton': 0.4248062015503876, 'part_isElectron': 0.008785529715762274, 'part_isMuon': 0.009302325581395349}\n"
     ]
    }
   ],
   "source": [
    "# Track incidence of particle types, feature ids 6-10\n",
    "\n",
    "for expert in parts_by_expert.keys():\n",
    "    part_types = {feature_ids[i]: 0 for i in range(6,11)}\n",
    "    for part in parts_by_expert[expert]:\n",
    "        for feature_id in range(6,11):\n",
    "            part_types[feature_ids[feature_id]] += part[feature_id]\n",
    "    for key in part_types.keys():\n",
    "        part_types[key] = part_types[key] / len(parts_by_expert[expert])\n",
    "    print(f'Expert {expert} particle types: {part_types}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9039846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track average pt_rel, e_rel, and std. devs\n",
    "\n",
    "for expert in parts_by_expert.keys():\n",
    "    pt_rels = []\n",
    "    e_rels = []\n",
    "    for part in parts_by_expert[expert]:\n",
    "        pt_rels.append(part[2])\n",
    "        e_rels.append(part[3])\n",
    "    pt_rels = np.array(pt_rels)\n",
    "    e_rels = np.array(e_rels)\n",
    "    print(f'Expert {expert} pt_rel: mean {np.mean(pt_rels)}, std. dev. {np.std(pt_rels)}')\n",
    "    print(f'Expert {expert} e_rel: mean {np.mean(e_rels)}, std. dev. {np.std(e_rels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3247f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from presoftmax attentions, search for induction heads\n",
    "# i.e. heads that reproduce previously processed particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc1235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes 10/16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8045521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of pt_rel for all experts\n",
    "plt.figure(figsize=(10,6))\n",
    "for expert in parts_by_expert.keys():\n",
    "    pt_rels = []\n",
    "    for part in parts_by_expert[expert]:\n",
    "        pt_rels.append(part[2])\n",
    "    plt.hist(pt_rels, bins=50, alpha=0.5, histtype='step', label=f'Expert {expert}', density=True)\n",
    "plt.xlabel('pt_rel')\n",
    "plt.ylabel('Density')\n",
    "plt.title(f'Distribution of pt_rel by Expert - Jet Type {jet_type}')\n",
    "plt.legend()\n",
    "#plt.savefig(f'/moe-interpretability-pv/plots/pt_rel_distribution_{jet_type.replace(\" \",\"_\")}_{jet_type}.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
